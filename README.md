# DKI - Dynamic KV Injection

> Attention-Level User Memory Plugin for Large Language Models

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[ç®€ä½“ä¸­æ–‡](README_CN.md) | English

## ğŸ“– Overview

DKI (Dynamic KV Injection) is an **LLM attention-level plugin** that dynamically injects user preferences and session history via Attention Hooks during inference, enabling cross-session personalized memory.

### What DKI Is

DKI is an **LLM plugin** designed specifically for **user-level memory**:

-   **Attention Hook Mechanism**: Injects K/V at the attention level via PyTorch Hooks, not prompt concatenation
-   **Configuration-Driven Adapter**: Automatically reads from upstream application databases, no code changes required
-   **Hybrid Injection Strategy**: Preference K/V injection (negative position) + History suffix prompt (positive position)

**Core Workflow**:

```
Upstream App â†’ Pass user_id + raw input â†’ DKI Plugin
    â†“
DKI reads upstream app database via config-driven adapter
    â†“
Preferences â†’ K/V injection (negative pos) | History â†’ suffix prompt (positive pos)
    â†“
Call LLM inference â†’ Return response
```

### What DKI Is NOT

-   **Not RAG**: DKI uses K/V injection, not prompt concatenation, doesn't consume token budget
-   **Not Knowledge Base Retrieval**: DKI focuses on user-level memory, use RAG for external knowledge
-   **No Interface Implementation Required**: Configuration-driven, upstream apps only pass user_id and raw input

### Why This Scope Matters

This focused scope enables:

1. **Short preferences** (50-200 tokens) â†’ reduced position encoding risks, cacheable
2. **User-owned data** â†’ simplified privacy considerations
3. **Session-coherent** â†’ effective K/V caching
4. **Stable preferences** â†’ high cache reuse rate

### Key Features

-   **ğŸ§  Attention Hook Injection**: Injects K/V at attention level via PyTorch Hooks, not prompt tokens
-   **ğŸ”€ Hybrid Injection Strategy**: Preferences (K/V negative position) + History (suffix prompt positive position)
-   **ğŸ”§ Configuration-Driven Adapter**: SQLAlchemy dynamic table mapping, no interface implementation required
-   **ğŸšï¸ Memory Influence Scaling (MIS)**: Continuous Î± âˆˆ [0, 1] control
-   **ğŸ”„ Query-Conditioned Projection**: FiLM-style memory-centric transformation
-   **ğŸš¦ Dual-Factor Gating**: Relevance-driven decision, entropy-modulated strength
-   **ğŸ’¾ Tiered KV Cache**: L1(GPU) â†’ L2(CPU) â†’ L3(SSD) â†’ L4(Recompute)
-   **ğŸ“Š Monitoring API**: Statistics, injection logs, health checks
-   **ğŸ”Œ Multi-Engine Support**: vLLM, LLaMA, DeepSeek, GLM
-   **âœ… Graceful Degradation**: Î± â†’ 0 smoothly recovers vanilla LLM behavior

## ğŸ—ï¸ Architecture

### Core Architecture: LLM Plugin Mode

DKI operates as an **attention-level plugin** for LLMs, implementing K/V injection via PyTorch Hook mechanism:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DKI Plugin Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Upstream Application (Chat UI / Customer Service / Other Apps) â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Only needs to pass: user_id + raw user input               â”‚    â”‚
â”‚  â”‚     (No RAG, No Prompt Engineering, No Interface Implementation)â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  DKI Plugin                                                     â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Config-Driven Adapter (SQLAlchemy dynamic table mapping)   â”‚    â”‚
â”‚  â”‚  â”‚   â””â”€â”€ Reads upstream app database (preferences + history)    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Preference Processing â†’ K/V Injection (negative pos, Hook) â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ History Processing â†’ Suffix Prompt (positive pos)          â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Monitoring API (stats/logs/health)                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  LLM Engine (vLLM / LLaMA / DeepSeek / GLM)                     â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Inference with K/V Injection                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Injection Strategy Selection

DKI provides two injection strategies, configurable via settings:

| Strategy             | Use Case   | Context Usage | Stability  | Research Value |
| -------------------- | ---------- | ------------- | ---------- | -------------- |
| **stable** (default) | Production | Medium        | â­â­â­â­â­ | â­â­           |
| **full_attention**   | Research   | Minimal       | â­â­â­     | â­â­â­â­â­     |

```yaml
# config.yaml
dki:
    injection_strategy: "stable" # stable | full_attention
```

### Hybrid Injection Strategy (Stable)

**Default strategy**, uses a **layered injection approach** that mirrors human cognition:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DKI Hybrid Injection Architecture (Stable)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 1: User Preferences (K/V Injection - Attention Hook)     â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Content: Dietary, style, interests                         â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Position: Negative (conceptually "before" user input)      â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Mechanism: PyTorch Hook modifies Attention K/V             â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Influence: Implicit, background (like personality)         â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Î±: 0.3-0.5 (lower, for subtle influence)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 2: Session History (Suffix Prompt)                       â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Content: Recent conversation turns                         â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Position: After user query (positive positions)            â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Mechanism: Standard token concatenation                    â”‚    â”‚
â”‚  â”‚  â”œâ”€â”€ Influence: Explicit, citable (like memory)                 â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Prompt: Trust-establishing guidance                        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 3: Current Query (Standard Input)                        â”‚    â”‚
â”‚  â”‚  â””â”€â”€ Primary focus of attention                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Full Attention / Engram-Inspired Strategy (âš ï¸ Deprecated)

> **Deprecation Notice**: The Full Attention strategy and Engram-Inspired Injection strategy have been deprecated. These strategies injected both preferences and history entirely via K/V to achieve near-zero context usage, but have **fundamental limitations with long history**:
>
> 1. **Limited K/V injection capacity**: As conversation history grows (tens to hundreds of turns), K/V token count increases dramatically, exceeding the effective attention range
> 2. **No explicit referenceability**: History injected via K/V cannot be explicitly referenced or reasoned about by the model
> 3. **OOD risk**: Massive K/V injection at negative positions causes severe distribution shift from training
> 4. **Poor factual accuracy**: The model cannot extract specific facts (dates, prices, etc.) from K/V-injected history
>
> **Replacement**: Use **Recall v4 Memory Recall Strategy** (see below), which provides stable and reliable memory recall for long history scenarios through multi-signal retrieval + dynamic summarization + application-layer fact supplementation.

<details>
<summary>Original Full Attention Strategy documentation (deprecated, click to expand)</summary>

Implemented in `dki/core/injection/full_attention_injector.py`.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DKI Full Attention Architecture (Research)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Position Layout:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [History KV]     â”‚  [Preference KV]   â”‚  [Query + Indication]  â”‚    â”‚
â”‚  â”‚  pos: -500~-101   â”‚  pos: -100~-1      â”‚  pos: 0~L              â”‚    â”‚
â”‚  â”‚  Î±: 0.3           â”‚  Î±: 0.4            â”‚  Î±: 1.0                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  Characteristics:                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  âœ… Minimal context usage (only 3-5 tokens global indication)  â”‚    â”‚
â”‚  â”‚  âœ… History also via K/V injection, no token budget consumed   â”‚    â”‚
â”‚  â”‚  âš ï¸ Potential OOD risk (requires experimental validation)      â”‚    â”‚
â”‚  â”‚  âš ï¸ History cannot be explicitly cited (implicit influence)    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  Research Objectives:                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. Validate feasibility of history K/V injection               â”‚    â”‚
â”‚  â”‚  2. Compare output quality with Stable strategy                 â”‚    â”‚
â”‚  â”‚  3. Collect attention pattern data                              â”‚    â”‚
â”‚  â”‚  4. Explore limits of 0% context usage                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Full Attention Injector Implementation

The `FullAttentionInjector` class (`dki/core/injection/full_attention_injector.py`) is the core implementation of the Full Attention Strategy. Key design details:

**Position Encoding Modes**: Three modes are supported, configurable via `position_mode`:

| Mode               | Description                                        | Use Case                        |
| ------------------ | -------------------------------------------------- | ------------------------------- |
| `fixed_negative`   | Memory tokens mapped to negative positions (RoPE)  | Default, clean separation       |
| `constant`         | All memory tokens share the same position           | Tests position-independence     |
| `nope`             | No position encoding applied to memory K/V          | Tests NoPE hypothesis           |

**Value-Only Scaling (Engram-Inspired)**: The injector applies Î± scaling exclusively to Value tensors, keeping Keys unscaled. This preserves attention addressing precision while modulating memory influence strengthâ€”a principle adopted from Engram [arXiv:2601.07372]:

```python
# Key (address) is NOT scaled â€” preserves matching precision
# Value (output contribution) IS scaled â€” modulates influence
h_v = h_v * history_alpha     # e.g., 0.3
p_v = p_v * preference_alpha  # e.g., 0.4
```

**K/V Merging**: History and preference K/V pairs are merged per-layer with history positioned further from the query (earlier negative positions) and preferences closer:

```
Merged layout per layer: [History K/V (far negative)] [Preference K/V (near negative)] [User K/V (positive)]
```

**Safety Mechanisms**:
- **Token limit**: If total K/V tokens exceed `max_total_kv_tokens` (default: 600), the injector either falls back to the Stable strategy or truncates history (most recent messages preserved)
- **Attention pattern logging**: When enabled, logs position distributions, token counts, and compute times for research analysis
- **Graceful fallback**: On any error, returns a non-injected result so the vanilla LLM can proceed normally

**Configuration Example**:

```yaml
dki:
    injection_strategy: "full_attention"

    full_attention:
        enabled: true
        position_mode: "fixed_negative" # fixed_negative | constant | nope

        preference:
            position_start: -100
            alpha: 0.4

        history:
            position_start: -500
            alpha: 0.3
            max_tokens: 400

        global_indication:
            enabled: true
            text_en: "[Memory Context Available]"
            text_cn: "[è®°å¿†ä¸Šä¸‹æ–‡å¯ç”¨]"
```

**Runtime Strategy Switching**:

```python
# Switch to full_attention strategy
dki.switch_injection_strategy("full_attention")

# Switch back to stable strategy
dki.switch_injection_strategy("stable")

# Get full_attention statistics
stats = dki.get_full_attention_stats()

# Get attention pattern logs (for research analysis)
logs = dki.get_full_attention_logs(limit=50)
```

</details>

### Recall v4 Memory Recall Strategy (Recommended)

**Production-recommended strategy**. Simulates human memory recall through multi-signal retrieval, dynamic history construction, and application-layer fact supplementation, providing stable and reliable memory capabilities for long history scenarios. Core implementation in `dki/core/recall/`.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DKI Recall v4 Memory Recall Architecture               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Phase 1: Multi-Signal Recall                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  User Input â†’ [Keywords+Weights] + [Anaphora] + [Vector Sim]   â”‚    â”‚
â”‚  â”‚            â†’  Weighted Merge + Normalization â†’ Message List     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                                              â”‚
â”‚  Phase 2: Dynamic History Construction                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Iterate messages:                                               â”‚    â”‚
â”‚  â”‚    Over threshold â†’ [SUMMARY] + trace_id (traceable)            â”‚    â”‚
â”‚  â”‚    Under threshold â†’ Original message                            â”‚    â”‚
â”‚  â”‚  + Recent N turns of complete conversation                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                          â†“                                              â”‚
â”‚  Phase 3: Model-Adaptive Assembly + Inference                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  [History Suffix] + [Trust+Reasoning Constraints] + [Pref K/V]  â”‚    â”‚
â”‚  â”‚  + [Query] â†’ LLM Inference â†’ Detect retrieve_fact call          â”‚    â”‚
â”‚  â”‚  â†’ Fact supplementation (chunked offset+limit) â†’ Continue       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â”‚  Advantages:                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  âœ… Stable and reliable for long history                         â”‚    â”‚
â”‚  â”‚  âœ… Facts are traceable (trace_id â†’ original message)            â”‚    â”‚
â”‚  â”‚  âœ… Dynamic context budget management                            â”‚    â”‚
â”‚  â”‚  âœ… Multi-model support (DeepSeek, GLM, Generic)                 â”‚    â”‚
â”‚  â”‚  âœ… Preferences still via K/V injection (reuses existing infra)  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration Example**:

```yaml
dki:
    injection_strategy: "recall_v4"

    recall:
        enabled: true
        strategy: "summary_with_fact_call"

        signals:
            keyword_enabled: true
            keyword_topk: 5
            keyword_method: "tfidf"
            vector_enabled: true
            vector_top_k: 10

        budget:
            generation_reserve: 512
            min_recent_turns: 2
            max_recent_turns: 5

        summary:
            per_message_threshold: 200
            strategy: "extractive"    # extractive (jieba TextRank) | llm

        fact_call:
            enabled: true
            max_rounds: 3
            max_fact_tokens: 800
```

**Runtime Strategy Switching**:

```python
# Switch to recall_v4 strategy
dki.switch_injection_strategy("recall_v4")

# Switch back to stable strategy
dki.switch_injection_strategy("stable")
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DKI Data Flow                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Query + user_id + session_id                                      â”‚
â”‚       â”‚                                                                 â”‚
â”‚       â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. Config-Driven Adapter reads upstream app database           â”‚    â”‚
â”‚  â”‚     â”œâ”€â”€ Preferences table â†’ Preference list                     â”‚    â”‚
â”‚  â”‚     â””â”€â”€ Messages table â†’ Relevant history (vector/BM25/keyword) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. Preference Processing                                       â”‚    â”‚
â”‚  â”‚     â”œâ”€â”€ Format preference text                                  â”‚    â”‚
â”‚  â”‚     â”œâ”€â”€ Compute/cache K/V representation                        â”‚    â”‚
â”‚  â”‚     â””â”€â”€ Prepare Attention Hook                                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. History Processing                                          â”‚    â”‚
â”‚  â”‚     â””â”€â”€ Format as suffix prompt (with trust guidance)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. LLM Inference (with K/V Injection)                          â”‚    â”‚
â”‚  â”‚     â”œâ”€â”€ Attention Hook injects preference K/V (negative pos)    â”‚    â”‚
â”‚  â”‚     â””â”€â”€ Input = query + history suffix (positive pos)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  5. Return response + Record monitoring data                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

> ğŸ“– **Complete Deployment Guide**: If you need to deploy DKI + AGA + vLLM from scratch on Ubuntu Server, see [DKI+AGA Complete Deployment Guide](docs/DKI_AGA_Complete_Deployment_Guide.md) for detailed environment setup, model download, service startup, and testing steps.

### Installation

```bash
# Clone the repository
cd DKI

# Setup (creates venv, installs dependencies, initializes DB)
# Windows:
scripts\setup.bat

# Linux/Mac:
chmod +x scripts/*.sh
./scripts/setup.sh
```

### Upstream Application Integration (Recommended)

DKI as an LLM plugin, upstream applications only need to:

1. **Provide adapter config file** - Specify database connection and field mapping
2. **Remove RAG/Prompt engineering code** - DKI handles it automatically
3. **Pass user_id + raw input** - No prompt construction needed

```python
from dki.core.dki_plugin import DKIPlugin
from dki.models.vllm_adapter import VLLMAdapter

# 1. Initialize LLM adapter
model_adapter = VLLMAdapter(model_name="Qwen/Qwen2-7B-Instruct")

# 2. Create DKI plugin from config (recommended)
# Upstream apps only need to provide config, no interface implementation
dki = await DKIPlugin.from_config(
    model_adapter=model_adapter,
    adapter_config_path="config/adapter_config.yaml",  # DB connection + field mapping
)

# 3. Call DKI - only pass user_id and raw input
# DKI will automatically:
# - Read user preferences from upstream app DB â†’ K/V injection
# - Retrieve relevant history messages â†’ suffix prompt
response = await dki.chat(
    query="Recommend a restaurant for tonight",  # Raw input, no prompt construction
    user_id="user_001",   # User ID (DKI uses to read preferences and history)
    session_id="session_001",  # Session ID (DKI uses to read session history)
)

print(response.text)
# Output considers:
# - Vegetarian preference (implicit, from K/V injection)
# - Previous restaurant visit (explicit, from history suffix)
# - Beijing location (implicit, from K/V injection)

# Monitoring data
print(f"Injection enabled: {response.metadata.injection_enabled}")
print(f"Alpha: {response.metadata.alpha}")
print(f"Preference tokens: {response.metadata.preference_tokens}")
print(f"History tokens: {response.metadata.history_tokens}")
print(f"Cache hit: {response.metadata.preference_cache_hit}")
print(f"Latency: {response.metadata.latency_ms}ms")
```

### Adapter Configuration Example

Create `config/adapter_config.yaml` to specify how to connect to upstream app database:

```yaml
user_adapter:
    # Database connection (connects to upstream app's database)
    database:
        type: postgresql # postgresql | mysql | sqlite
        host: localhost
        port: 5432
        database: my_app_db # Upstream app's database
        username: user
        password: pass

    # Preferences table mapping (maps to upstream app's table structure)
    preferences:
        table: user_preferences # Upstream app's table name
        fields:
            user_id: user_id # Upstream app's field name
            preference_text: content
            preference_type: type
            priority: priority

    # Messages table mapping
    messages:
        table: chat_messages
        fields:
            message_id: id
            session_id: session_id
            user_id: user_id
            role: role
            content: content
            timestamp: created_at

        # JSON Content Extraction (Important!)
        # If content field stores JSON strings (e.g., raw AI responses)
        # specify a JSON key to extract the actual text content
        #
        # Scenario: Upstream app stores raw AI response, content might be:
        #   '{"text": "Restaurant recommendation", "model": "gpt-4", "tokens": 100}'
        #
        # With content_json_key: "text", DKI extracts "Restaurant recommendation"
        # If JSON parsing fails or key not found, uses raw content (safe fallback)
        content_json_key:
            null # Set to JSON key name, e.g., "text", "content"
            # Supports nesting: "data.text", "choices.0.message.content"

    # Vector search config (supports dynamic vector processing)
    vector_search:
        type: dynamic # pgvector | faiss | dynamic
        dynamic:
            strategy: hybrid # lazy | batch | hybrid (BM25 + embedding)
```

#### JSON Content Extraction

Many applications store raw AI responses in the database, where the `content` field might be a JSON string:

```json
{
    "text": "I recommend the Sichuan restaurant",
    "model": "gpt-4",
    "tokens": 50,
    "finish_reason": "stop"
}
```

By configuring `content_json_key`, DKI can automatically extract the actual text content:

| Config Value                  | JSON Data                      | Extracted Result     |
| ----------------------------- | ------------------------------ | -------------------- |
| `"text"`                      | `{"text": "Hello"}`            | `"Hello"`            |
| `"data.text"`                 | `{"data": {"text": "Nested"}}` | `"Nested"`           |
| `"choices.0.message.content"` | OpenAI format response         | Actual reply content |

**Safe Fallback**: If JSON parsing fails or the specified key doesn't exist, DKI uses the raw content without errors.

### Example Chat UI

DKI provides a Vue3 + Element Plus example Chat UI to demonstrate DKI integration:

```bash
# Start both frontend and backend
python start_dev.py

# Start backend only
python start_dev.py backend

# Start frontend only
python start_dev.py frontend
```

**UI Features**:

-   ğŸ” User login/registration
-   ğŸ’¬ Chat interface with Markdown rendering
-   âš™ï¸ User preference management (CRUD)
-   ğŸ“Š Session history management
-   ğŸ“ˆ System statistics monitoring (requires admin password)
-   ğŸ¨ Light/dark theme toggle

**Note**: Chat UI is an **example application** to demonstrate DKI integration. DKI's adapter reads the Chat UI's database to get user preferences and history messages.

### Monitoring API

DKI provides monitoring API to view internal working status:

```python
# Get statistics
stats = dki.get_stats()
print(f"Total requests: {stats['total_requests']}")
print(f"Injection rate: {stats['injection_rate']:.2%}")
print(f"Cache hit rate: {stats['cache_hit_rate']:.2%}")
print(f"Avg latency: {stats['avg_latency_ms']:.1f}ms")

# Get injection logs
logs = dki.get_injection_logs(limit=10)
for log in logs:
    print(f"[{log['timestamp']}] alpha={log['alpha']:.2f}, enabled={log['injection_enabled']}")
```

REST API endpoints:

-   `GET /v1/dki/info` - Get DKI plugin status
-   `POST /v1/dki/chat` - DKI enhanced chat (upstream apps call this endpoint)

## ğŸ“ Project Structure

```
DKI/
â”œâ”€â”€ config/                              # Configuration files
â”‚   â”œâ”€â”€ config.yaml                      # â­ Main configuration
â”‚   â”œâ”€â”€ adapter_config.example.yaml      # â­ Adapter config example
â”‚   â”œâ”€â”€ memory_trigger.yaml              # Memory Trigger config
â”‚   â””â”€â”€ reference_resolver.yaml          # Reference Resolver config
â”‚
â”œâ”€â”€ dki/                                 # Core code directory
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                            # â­ Core modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dki_plugin.py                # â­ DKI Plugin Core (entry point)
â”‚   â”‚   â”œâ”€â”€ dki_system.py                # DKI System wrapper
â”‚   â”‚   â”œâ”€â”€ architecture.py              # Architecture definitions
â”‚   â”‚   â”œâ”€â”€ plugin_interface.py          # Plugin interface definitions
â”‚   â”‚   â”œâ”€â”€ memory_router.py             # FAISS-based vector retrieval
â”‚   â”‚   â”œâ”€â”€ embedding_service.py         # Embedding computation service
â”‚   â”‚   â”œâ”€â”€ rag_system.py               # RAG baseline (for comparison)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ injection/                   # â­ Injection strategies
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ full_attention_injector.py  # Full Attention strategy (research)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ components/                  # â­ Core algorithm components
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ memory_influence_scaling.py    # MIS - Memory Influence Scaling
â”‚   â”‚       â”œâ”€â”€ query_conditioned_projection.py  # QCP - Query Conditioned Projection
â”‚   â”‚       â”œâ”€â”€ dual_factor_gating.py          # Dual-Factor Gating decision
â”‚   â”‚       â”œâ”€â”€ hybrid_injector.py             # Hybrid injector
â”‚   â”‚       â”œâ”€â”€ memory_trigger.py              # â­ Memory trigger detection
â”‚   â”‚       â”œâ”€â”€ reference_resolver.py          # â­ Reference resolver
â”‚   â”‚       â”œâ”€â”€ attention_budget.py            # Attention budget tracking
â”‚   â”‚       â”œâ”€â”€ session_kv_cache.py            # Session-level K/V cache
â”‚   â”‚       â”œâ”€â”€ tiered_kv_cache.py             # L1/L2/L3/L4 tiered cache
â”‚   â”‚       â””â”€â”€ position_remapper.py           # Position encoding remapping (RoPE/ALiBi)
â”‚   â”‚
â”‚   â”œâ”€â”€ adapters/                        # â­ External data adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                      # Adapter abstract base class
â”‚   â”‚   â”œâ”€â”€ config_driven_adapter.py     # â­ Config-driven adapter (core)
â”‚   â”‚   â”œâ”€â”€ factory.py                   # Adapter factory
â”‚   â”‚   â”œâ”€â”€ example_adapter.py           # Example adapter
â”‚   â”‚   â”œâ”€â”€ memory_adapter.py            # In-memory adapter
â”‚   â”‚   â”œâ”€â”€ postgresql_adapter.py        # PostgreSQL adapter
â”‚   â”‚   â”œâ”€â”€ mysql_adapter.py             # MySQL adapter
â”‚   â”‚   â”œâ”€â”€ mongodb_adapter.py           # MongoDB adapter
â”‚   â”‚   â”œâ”€â”€ redis_adapter.py             # Redis adapter
â”‚   â”‚   â””â”€â”€ rest_adapter.py              # REST API adapter
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/                       # â­ FlashAttention integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py                    # FlashAttention configuration
â”‚   â”‚   â”œâ”€â”€ backend.py                   # Backend detection (FA3/FA2/Standard)
â”‚   â”‚   â”œâ”€â”€ kv_injection.py              # Optimized K/V injection computation
â”‚   â”‚   â””â”€â”€ profiler.py                  # Performance profiler
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                             # REST API routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dki_routes.py                # â­ DKI Chat API
â”‚   â”‚   â”œâ”€â”€ visualization_routes.py      # â­ Injection visualization API
â”‚   â”‚   â”œâ”€â”€ stats_routes.py              # Statistics API
â”‚   â”‚   â”œâ”€â”€ monitoring_routes.py         # Monitoring API
â”‚   â”‚   â”œâ”€â”€ auth_routes.py               # Authentication API
â”‚   â”‚   â”œâ”€â”€ session_routes.py            # Session management API
â”‚   â”‚   â”œâ”€â”€ preference_routes.py         # Preference management API
â”‚   â”‚   â”œâ”€â”€ routes.py                    # Route registration
â”‚   â”‚   â”œâ”€â”€ dependencies.py              # Dependency injection
â”‚   â”‚   â””â”€â”€ models.py                    # API data models
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                          # LLM model adapters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ factory.py                   # Model factory
â”‚   â”‚   â”œâ”€â”€ base.py                      # Base adapter (with FlashAttention)
â”‚   â”‚   â”œâ”€â”€ vllm_adapter.py              # vLLM adapter
â”‚   â”‚   â”œâ”€â”€ llama_adapter.py             # LLaMA adapter
â”‚   â”‚   â”œâ”€â”€ deepseek_adapter.py          # DeepSeek adapter
â”‚   â”‚   â””â”€â”€ glm_adapter.py              # GLM adapter
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/                           # â­ Cache system
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preference_cache.py          # â­ Preference cache manager (L1+L2)
â”‚   â”‚   â”œâ”€â”€ redis_client.py              # â­ Redis distributed cache client
â”‚   â”‚   â””â”€â”€ non_vectorized_handler.py    # Dynamic vector processing (BM25+Embedding)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                          # Configuration loading
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config_loader.py             # YAML config loader
â”‚   â”‚
â”‚   â”œâ”€â”€ database/                        # Database
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py                    # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ connection.py                # Database connection manager
â”‚   â”‚   â””â”€â”€ repository.py               # Data repository
â”‚   â”‚
â”‚   â”œâ”€â”€ experiment/                      # Experiment system
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ runner.py                    # Experiment runner (DKI/RAG/Baseline)
â”‚   â”‚   â”œâ”€â”€ metrics.py                   # Evaluation metrics (recall/hallucination/latency)
â”‚   â”‚   â””â”€â”€ data_generator.py            # Test data generation
â”‚   â”‚
â”‚   â”œâ”€â”€ example_app/                     # Example integration app
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py                       # Example FastAPI app
â”‚   â”‚   â”œâ”€â”€ main.py                      # Example entry point
â”‚   â”‚   â””â”€â”€ service.py                   # Example business logic
â”‚   â”‚
â”‚   â””â”€â”€ web/                             # Web application
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ app.py                       # FastAPI main application
â”‚
â”œâ”€â”€ ui/                                  # â­ Vue3 Example Frontend UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.vue                      # Root component
â”‚   â”‚   â”œâ”€â”€ main.ts                      # Entry file
â”‚   â”‚   â”œâ”€â”€ vite-env.d.ts
â”‚   â”‚   â”œâ”€â”€ views/                       # Page components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatView.vue             # ğŸ’¬ Chat page (Markdown rendering)
â”‚   â”‚   â”‚   â”œâ”€â”€ InjectionVizView.vue     # ğŸ“Š Injection visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ PreferencesView.vue      # âš™ï¸ Preferences management
â”‚   â”‚   â”‚   â”œâ”€â”€ SessionsView.vue         # ğŸ“‹ Session management
â”‚   â”‚   â”‚   â”œâ”€â”€ StatsView.vue            # ğŸ“ˆ Statistics monitoring
â”‚   â”‚   â”‚   â””â”€â”€ LoginView.vue            # ğŸ” Login page
â”‚   â”‚   â”œâ”€â”€ components/                  # Common components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.vue            # Chat input box
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageItem.vue          # Message bubble
â”‚   â”‚   â”‚   â””â”€â”€ SettingsDialog.vue       # Settings dialog
â”‚   â”‚   â”œâ”€â”€ layouts/
â”‚   â”‚   â”‚   â””â”€â”€ MainLayout.vue           # Main layout
â”‚   â”‚   â”œâ”€â”€ stores/                      # Pinia state management
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.ts                  # Auth state
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.ts                  # Chat state
â”‚   â”‚   â”‚   â”œâ”€â”€ preferences.ts           # Preferences state
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.ts              # Settings state
â”‚   â”‚   â”‚   â””â”€â”€ statsAuth.ts             # Stats auth state
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts                   # API service wrapper
â”‚   â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”‚   â””â”€â”€ index.ts                 # Vue Router routes
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â””â”€â”€ index.ts                 # Frontend config
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ index.ts                 # TypeScript type definitions
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ markdown.ts              # Markdown rendering utility
â”‚   â”‚   â””â”€â”€ assets/styles/
â”‚   â”‚       â”œâ”€â”€ main.scss                # Main styles
â”‚   â”‚       â””â”€â”€ variables.scss           # Style variables
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tsconfig.node.json
â”‚   â”œâ”€â”€ env.example
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/                                # ğŸ“š Documentation
â”‚   â”œâ”€â”€ DKI_AGA_Complete_Deployment_Guide.md  # â­ DKI+AGA Full Deployment Guide
â”‚   â”œâ”€â”€ DKI_Architecture_Diagrams.md     # â­ Architecture & flow diagrams
â”‚   â”œâ”€â”€ DKI_Optimization_Roadmap.md      # â­ Optimization plan & productization
â”‚   â”œâ”€â”€ Integration_Guide.md             # Integration guide
â”‚   â”œâ”€â”€ Dynamic_Vector_Search.md         # Dynamic vector search docs
â”‚   â”œâ”€â”€ FlashAttention3_Integration.md   # FlashAttention integration plan
â”‚   â”œâ”€â”€ DKI_Plugin_Architecture.md       # Plugin architecture documentation
â”‚   â””â”€â”€ Chat_UI_è®¾è®¡æ–¹æ¡ˆ.md              # UI design document
â”‚
â”œâ”€â”€ tests/                               # ğŸ§ª Tests
â”‚   â”œâ”€â”€ unit/                            # Unit tests
â”‚   â”‚   â”œâ”€â”€ test_dki_plugin.py           # DKI plugin tests
â”‚   â”‚   â”œâ”€â”€ test_config_driven_adapter.py # Adapter tests
â”‚   â”‚   â”œâ”€â”€ test_json_content_extraction.py # JSON parsing tests
â”‚   â”‚   â”œâ”€â”€ test_memory_trigger.py       # Memory trigger tests
â”‚   â”‚   â”œâ”€â”€ test_reference_resolver.py   # Reference resolver tests
â”‚   â”‚   â”œâ”€â”€ test_flash_attention.py      # FlashAttention tests
â”‚   â”‚   â”œâ”€â”€ test_redis_cache.py          # Redis cache tests
â”‚   â”‚   â”œâ”€â”€ components/                  # Component unit tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_attention_budget.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_dual_factor_gating.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_memory_influence_scaling.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_position_remapper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_query_conditioned_projection.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_session_kv_cache.py
â”‚   â”‚   â”‚   â””â”€â”€ test_tiered_kv_cache.py
â”‚   â”‚   â”œâ”€â”€ core/                        # Core module tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_dki_system.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_embedding_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_memory_router.py
â”‚   â”‚   â”‚   â””â”€â”€ test_rag_baseline.py
â”‚   â”‚   â””â”€â”€ database/                    # Database tests
â”‚   â”‚       â”œâ”€â”€ test_connection.py
â”‚   â”‚       â””â”€â”€ test_repository.py
â”‚   â”œâ”€â”€ integration/                     # Integration tests
â”‚   â”‚   â”œâ”€â”€ test_dki_chat_flow.py
â”‚   â”‚   â”œâ”€â”€ test_dki_vs_rag.py
â”‚   â”‚   â”œâ”€â”€ test_kv_injection_flow.py
â”‚   â”‚   â””â”€â”€ test_cache_eviction_flow.py
â”‚   â”œâ”€â”€ behavior/                        # Behavior tests
â”‚   â”‚   â”œâ”€â”€ test_budget_enforcement.py
â”‚   â”‚   â”œâ”€â”€ test_influence_monotonicity.py
â”‚   â”‚   â””â”€â”€ test_injection_isolation.py
â”‚   â””â”€â”€ fixtures/                        # Test fixtures
â”‚       â”œâ”€â”€ fake_attention.py
â”‚       â”œâ”€â”€ fake_embeddings.py
â”‚       â”œâ”€â”€ fake_model.py
â”‚       â””â”€â”€ sample_memories.py
â”‚
â”œâ”€â”€ scripts/                             # Scripts
â”‚   â”œâ”€â”€ setup.bat / setup.sh             # Setup scripts
â”‚   â”œâ”€â”€ start.bat / start.sh             # Start scripts
â”‚   â””â”€â”€ init_db.sql                      # Database initialization
â”‚
â”œâ”€â”€ start_dev.py                         # â­ Dev startup script (frontend + backend)
â”œâ”€â”€ main.py                              # â­ Main entry point (CLI)
â”œâ”€â”€ requirements.txt                     # Python dependencies
â”œâ”€â”€ setup.py                             # Installation config
â”œâ”€â”€ QUICKSTART.md                        # Quick start guide
â”œâ”€â”€ README_CN.md                         # Chinese documentation
â””â”€â”€ README.md                            # English documentation
```

## ğŸ“Š Project Status

| Module                  | Status     | Description                                |
| ----------------------- | ---------- | ------------------------------------------ |
| DKI Core Plugin         | âœ… Done    | K/V injection, hybrid strategy, gating     |
| Full Attention Strategy | âš ï¸ Deprecated | Deprecated: limited by long history scenarios |
| Recall v4 Memory Recall | âœ… Done    | Multi-signal retrieval + dynamic summary + fact call |
| Config-Driven Adapter   | âœ… Done    | SQLAlchemy dynamic table mapping           |
| JSON Content Extraction | âœ… Done    | Auto-parse JSON content fields             |
| Memory Trigger          | âœ… Done    | Memory trigger detection, configurable     |
| Reference Resolver      | âœ… Done    | Reference parsing, configurable recall     |
| Redis Distributed Cache | âœ… Done    | L1+L2 cache, multi-instance support        |
| FlashAttention          | âœ… Done    | FA3/FA2 auto-detection, graceful fallback  |
| Injection Visualization | âœ… Done    | Flow diagram, token distribution, history  |
| Vue3 Example UI         | âœ… Done    | Chat, preferences, stats, visualization    |
| Monitoring API          | âœ… Done    | Statistics, logs, health check             |
| Architecture Diagrams   | âœ… Done    | System architecture & injection flow docs  |
| Unit Tests              | âœ… Done    | Core component test coverage               |
| Attention Heatmap       | ğŸ”„ Planned | Debug attention weight visualization       |
| LangChain/LlamaIndex    | ğŸ”„ Planned | Ecosystem integration                      |
| Multimodal Memory       | ğŸ“‹ TBD     | Image/audio memory support                 |

## âš™ï¸ Configuration

### DKI Main Configuration

Edit `config/config.yaml`:

```yaml
# Model Engine
model:
    default_engine: "vllm" # vllm, llama, deepseek, glm
    engines:
        vllm:
            model_name: "Qwen/Qwen2-7B-Instruct"
            tensor_parallel_size: 1

# DKI Plugin Settings
dki:
    enabled: true
    version: "2.5"

    # Hybrid Injection Strategy
    hybrid_injection:
        enabled: true
        language: "cn" # en | cn

        # Preferences: K/V injection (Attention Hook, negative position)
        preference:
            enabled: true
            position_strategy: "negative"
            alpha: 0.4 # Lower for background influence
            max_tokens: 200

        # History: Suffix prompt (positive position)
        history:
            enabled: true
            method: "suffix_prompt"
            max_tokens: 2000
            max_messages: 10

    # Gating: Relevance-driven, entropy-modulated
    gating:
        relevance_threshold: 0.7
        entropy_ceiling: 1.0
        entropy_floor: 0.5

    # Safety
    safety:
        max_alpha: 0.8
        fallback_on_error: true
        audit_logging: true
```

### Adapter Configuration (Core)

Create `config/adapter_config.yaml` to configure how to connect to upstream app database:

```yaml
user_adapter:
    # Database connection - connects to upstream app's database
    database:
        type: postgresql # postgresql | mysql | sqlite
        host: localhost
        port: 5432
        database: my_app_db # Upstream app's database name
        username: user
        password: pass
        pool_size: 5

    # Preferences table mapping - maps to upstream app's table structure
    preferences:
        table: user_preferences # Upstream app's preference table name
        fields:
            user_id: user_id # Field mapping: DKI field -> upstream app field
            preference_text: content
            preference_type: type
            priority: priority
            created_at: created_at
        filters:
            is_active: true # Additional filter conditions

    # Messages table mapping
    messages:
        table: chat_messages # Upstream app's message table name
        fields:
            message_id: id
            session_id: session_id
            user_id: user_id
            role: role
            content: content
            timestamp: created_at
            embedding: embedding # Vector field (optional)

    # Vector search configuration
    vector_search:
        enabled: true
        type: dynamic # pgvector | faiss | dynamic | none

        # Dynamic vector processing (used when no pre-computed vectors)
        dynamic:
            strategy: hybrid # lazy | batch | hybrid
            # hybrid = BM25 initial filtering + embedding reranking

        # Retrieval parameters
        top_k: 10
        similarity_threshold: 0.5

    # Cache configuration
    cache_enabled: true
    cache_ttl: 300 # 5 minutes
```

### Configuration Notes

**Core philosophy of adapter configuration**:

-   Upstream apps **don't need to implement any interface**
-   Just provide config file specifying database connection and field mapping
-   DKI uses SQLAlchemy to dynamically reflect table structure
-   Supports PostgreSQL, MySQL, SQLite and other mainstream databases

## ğŸ“Š Experiments

### Generate Test Data

```bash
python -m dki.experiment.data_generator
```

### Run Comparison Experiment

```python
from dki.experiment.runner import ExperimentRunner, ExperimentConfig

runner = ExperimentRunner()
config = ExperimentConfig(
    name="DKI vs RAG Comparison",
    modes=["dki", "rag", "baseline"],
    datasets=["persona_chat", "memory_qa"],
    max_samples=100
)

results = runner.run_experiment(config)
```

### Alpha Sensitivity Analysis

```python
results = runner.run_alpha_sensitivity(
    alpha_values=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
)
```

## ğŸ“ˆ API Reference

### DKI Plugin API

| Endpoint       | Method | Description                                 |
| -------------- | ------ | ------------------------------------------- |
| `/v1/dki/chat` | POST   | DKI enhanced chat (upstream apps call this) |
| `/v1/dki/info` | GET    | Get DKI plugin status                       |

### Monitoring API

| Endpoint         | Method | Description           |
| ---------------- | ------ | --------------------- |
| `/api/stats`     | GET    | Get system statistics |
| `/api/stats/dki` | GET    | Get DKI statistics    |
| `/api/health`    | GET    | Health check          |

### DKI Chat Request

Upstream apps only need to pass `user_id` and raw input:

```json
{
    "query": "Recommend a restaurant",
    "user_id": "user_001",
    "session_id": "session_001",
    "temperature": 0.7,
    "max_tokens": 512
}
```

### DKI Chat Response

```json
{
    "id": "dki-abc12345",
    "text": "Based on your preference for vegetarian food...",
    "input_tokens": 128,
    "output_tokens": 256,
    "dki_metadata": {
        "injection_enabled": true,
        "alpha": 0.4,
        "preference_tokens": 85,
        "history_tokens": 320,
        "cache_hit": true,
        "cache_tier": "memory",
        "latency_ms": 156.3
    },
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "Based on your preference..."
            },
            "finish_reason": "stop"
        }
    ],
    "created": 1707523200
}
```

## ğŸ”¬ Research Background

### DKI's Positioning: User-Level Memory System

Unlike RAG which targets **external knowledge** (documents, databases, web content), DKI is designed specifically for **user-level memory**:

| Dimension            | RAG                            | DKI                                                     |
| -------------------- | ------------------------------ | ------------------------------------------------------- |
| **Target Data**      | External knowledge bases       | User preferences, session history                       |
| **Data Size**        | Large (thousands of documents) | Small to Medium (prefs 50-200, history 100-4000 tokens) |
| **Update Frequency** | Batch updates                  | Real-time per session                                   |
| **Privacy**          | Shared knowledge               | User-owned data                                         |
| **Caching**          | Document-level                 | User-level (high reuse)                                 |

> **Note**: DKI token count depends on session complexity and relevance. Preference injection should stay short (50-200 tokens), while history injection can extend to 4000+ tokens as needed. For long histories, enable relevance filtering to optimize performance.

This focused scope is **intentional** and enables DKI's key advantages.

### Token Budget Analysis

DKI addresses a fundamental limitation of RAG: retrieved content consumes context window capacity.

**RAG Paradigm:**

```
[Retrieved Content (consumed)] [User Input (remaining)]
Token Budget: B_t^used = n_m + n_u
Attention Budget: B_a = (n_m + n_u)Â²
```

**DKI Paradigm:**

```
[User Input (full budget available)]
     â†‘ Memory injected via K/V (not in token budget)
Token Budget: B_t^used = n_u (memory free!)
Attention Budget: B_a = n_u Ã— (n_m + n_u)
```

### DKI vs Cross-Attention

DKI is NOT equivalent to Cross-Attention:

| Feature       | DKI                  | Cross-Attention                          |
| ------------- | -------------------- | ---------------------------------------- |
| Parameters    | Reuses W_k, W_v      | Separate W_q^cross, W_k^cross, W_v^cross |
| Training      | Training-free        | Requires training                        |
| Architecture  | No modification      | Dedicated layers                         |
| Compatibility | Any decoder-only LLM | Encoder-decoder only                     |
| Control       | Continuous Î±         | Learned weights                          |

### Hybrid Injection Rationale

Why use different strategies for preferences vs history?

| Memory Type     | Characteristics                               | Strategy                          | Reason                                      |
| --------------- | --------------------------------------------- | --------------------------------- | ------------------------------------------- |
| **Preferences** | Short (50-200 tokens), stable, abstract       | K/V injection (negative position) | Low OOD risk, cacheable, implicit influence |
| **History**     | Variable (100-4000 tokens), dynamic, concrete | Suffix prompt (positive position) | Zero OOD risk, citable, explicit reference  |

This layered approach:

-   Minimizes OOD risk (preferences are short)
-   Enables history citation (visible in prompt)
-   Reduces hallucination (trust-establishing prompts)

### Token Count and Performance Impact

DKI's supported token range depends on session complexity and relevance:

| Token Range | Use Case                              | Latency Impact | VRAM (7B model) |
| ----------- | ------------------------------------- | -------------- | --------------- |
| 100-500     | Simple prefs + short history          | < 10%          | ~250MB          |
| 500-2000    | Medium complexity sessions            | 10-30%         | ~1GB            |
| 2000-4000   | Complex multi-turn sessions           | 30-50%         | ~2GB            |
| 4000+       | Long sessions (filtering recommended) | > 50%          | > 2GB           |

**Performance Optimization Tips**:

-   For long histories, enable `search_relevant_history` for relevance filtering
-   Keep preferences short (50-200 tokens) to leverage K/V caching
-   History uses suffix prompts, length can be dynamically adjusted

**VRAM Comparison with RAG**:

-   For the same token count, DKI and RAG have **similar VRAM usage**
-   DKI may use more VRAM due to caching multi-turn history
-   However, DKI's K/V cache is reusable, no recomputation for subsequent requests

### Design Invariants

1. **Storage Model-Agnostic**: Store only original text + routing vectors
2. **Injection Model-Consistent**: K/V computed with target model parameters
3. **Session Cache Disposable**: Inference-time enhancement, not persistent
4. **Graceful Degradation**: Î± â†’ 0 falls back to vanilla LLM
5. **Audit Logging**: All injection decisions logged for compliance

### Memory Hierarchy (Tiered KV Cache)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L1: GPU HBM (Hot)     - Uncompressed FP16         â”‚
â”‚  L2: CPU RAM (Warm)    - Compressed (2-4Ã—)         â”‚
â”‚  L3: NVMe SSD (Cold)   - Quantized INT8 (8Ã—)       â”‚
â”‚  L4: Text Only         - Recompute on demand       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Memory footprint scales with ACTIVE memories, not total corpus size.

## ğŸ“š Detailed Documentation

### Core Concepts

#### 1. Attention Budget Reallocation Hypothesis

**Hypothesis Statement**: In reasoning-intensive tasks, the marginal benefit of releasing token budget exceeds the marginal cost of increased attention computation.

**Mathematical Formulation**:

```
âˆ‚TaskSuccess/âˆ‚B_t^free > âˆ‚Latency/âˆ‚B_a
```

**Intuition**:

-   Token budget is a **hard constraint** (truncation causes information loss)
-   Attention budget is a **soft constraint** (increased computation, but no information loss)
-   For tasks requiring deep reasoning chains (multi-step math, complex planning), preserving token budget for reasoning steps provides greater utility than attention latency cost

#### 2. Memory Influence Scaling (MIS)

Continuous strength control Î± âˆˆ [0, 1]:

| Î± Value     | Behavior                          | Use Case               |
| ----------- | --------------------------------- | ---------------------- |
| 0.0         | No memory influence (vanilla LLM) | Safe fallback          |
| 0.2-0.4     | Gentle injection                  | Exploratory dialogue   |
| **0.4-0.7** | **Optimal range**                 | **Most scenarios**     |
| 0.8-1.0     | Strong injection                  | High-confidence memory |

**Implementation**: Scaling applied via pre-softmax logit bias:

```python
logit_bias = torch.log(torch.tensor(alpha + 1e-9))
logits_mem_scaled = logits_mem + logit_bias
```

#### 3. Query-Conditioned Projection (QCP)

Query-dependent memory projection using FiLM-style modulation:

```python
# Generate modulation parameters
gamma = gamma_net(query_context)  # Scale
beta = beta_net(query_context)    # Shift

# Low-rank projection with FiLM modulation
X_mem_low = X_mem @ W_mem
X_mem_modulated = X_mem_low * gamma + beta
X_mem_proj = proj_out(X_mem_modulated)

# Residual connection
return layer_norm(X_mem + X_mem_proj)
```

**Key Principle**: Projection is **memory-centric**, not query-centricâ€”query only modulates, never re-encodes memory semantics.

#### 4. Dual-Factor Gating

**Design Decision**: Injection decision is **relevance-driven**; uncertainty only **modulates the upper bound of Î±**.

```python
# Factor 1: Memory relevance (PRIMARY)
inject = similarity_top1 > threshold_relevance

# Factor 2: Entropy modulates Î± ceiling (not decision)
alpha_max = entropy_floor + (entropy_ceiling - entropy_floor) * entropy

# Continuous strength
alpha = min(alpha_base, alpha_max)
```

**Important Note**: We use attention entropy as a **heuristic proxy** for model uncertainty, not a rigorous uncertainty estimator.

### Comparison with RAG

| Dimension          | RAG                                 | DKI                                       |
| ------------------ | ----------------------------------- | ----------------------------------------- |
| Injection Level    | Token domain (prompt concatenation) | Attention domain (K/V injection)          |
| Injection Control  | None (hard concatenation)           | Continuous (Î± âˆˆ [0, 1])                   |
| Prompt Consumption | Yes                                 | No                                        |
| Context Window     | Consumed by retrieved content       | Fully available to user                   |
| Prompt Engineering | Required                            | Simplified                                |
| Interpretability   | High (visible in prompt)            | Medium (requires attention visualization) |
| Ecosystem Maturity | High (LangChain, LlamaIndex, etc.)  | Low (emerging)                            |
| Debugging          | Straightforward                     | Requires specialized tools                |

### Applicable Scenarios

#### âœ… Recommended for DKI

1. **Personalized Assistants**

    - User preferences need to persist across sessions
    - Implicit personalization (no explicit mention in prompt)
    - Multi-turn dialogue with context continuity

2. **Customer Service Systems**

    - User profile + conversation history
    - Consistent experience across sessions
    - Privacy-sensitive user data

3. **Educational Applications**

    - Learning preferences + progress history
    - Adaptive responses based on user level
    - Long-term user modeling

4. **Health/Wellness Assistants**
    - Health profile + consultation history
    - Sensitive personal data
    - Consistent medical context

#### âš ï¸ Use RAG Instead

1. **External Knowledge Retrieval**

    - Document search and QA
    - Public knowledge bases
    - Frequently updated content

2. **First-turn Latency Critical**

    - First interaction must be fastest
    - Cannot accept gating overhead
    - Cold start performance critical

3. **High Audit Requirements**

    - Need to display full retrieved content
    - Regulators require visible prompts
    - Attention visualization insufficient

4. **Rapid Prototyping**
    - Using mature RAG ecosystem (LangChain, LlamaIndex)
    - Need fast iteration
    - No deep customization needed

### Performance Benchmarks

Based on DeepSeek-V3 7B experiments (n=500):

| Metric                 | RAG    | DKI        | Change     |
| ---------------------- | ------ | ---------- | ---------- |
| Memory Recall          | 87.3%  | 86.2%      | -1.1%      |
| First Turn Latency     | 78.8ms | 92.4ms     | +17.3%     |
| **Subsequent Latency** | 76.1ms | **42.8ms** | **-43.7%** |
| Cache Hit Rate         | N/A    | 69.7%      | -          |
| Hallucination Rate     | 10.2%  | 10.4%      | +0.2%      |

**Key Findings**:

-   DKI has 17.3% overhead on first turn, but 43.7% reduction on subsequent turns
-   After 3 turns total latency: DKI < RAG (178.2ms vs 228.9ms)
-   Memory recall only drops 1.1%, hallucination rate comparable

### Failure Modes and Mitigation

| Failure Mode           | Symptoms                                     | Detection                             | Mitigation                            |
| ---------------------- | -------------------------------------------- | ------------------------------------- | ------------------------------------- |
| Memory Confusion       | Multiple similar memories cause mixed output | Compute inter-memory similarity       | Keep only top-1 when similarity > 0.9 |
| Temporal Inconsistency | Old memory conflicts with current context    | Check memory timestamp vs query tense | Add time decay factor to Î±            |
| Projection Overfitting | Good training, poor test performance         | Monitor validation vs training loss   | Increase dropout, data augmentation   |
| Cache Thrashing        | Hit rate < 30%, latency increases            | Monitor cache hit rate                | Increase cache size or use LFU        |
| Bias Amplification     | Memory injection amplifies model biases      | Monitor output diversity metrics      | Implement diversity-aware routing     |

### Advanced Features

#### Tiered KV Cache

```python
from dki.core.components.tiered_kv_cache import TieredKVCache

cache = TieredKVCache(
    l1_size_gb=8,      # GPU
    l2_size_gb=32,     # CPU RAM
    l3_size_gb=128,    # SSD
    compression_l2=4,   # L2 compression ratio
    compression_l3=8    # L3 compression ratio
)

# Automatic tiered management
kv_pair = cache.get_or_compute(memory_id, compute_fn)
```

#### Attention Budget Tracking

```python
from dki.core.components.attention_budget import AttentionBudgetTracker

tracker = AttentionBudgetTracker()
with tracker.track_request(session_id):
    response = dki.chat(query, session_id)

# Get budget usage
budget = tracker.get_budget_usage(session_id)
print(f"Token budget used: {budget.token_used}/{budget.token_total}")
print(f"Attention FLOPs: {budget.attention_flops}")
```

### FAQ

**Q: Does DKI require retraining the model?**  
A: No. DKI is an inference-time enhancement using frozen model parameters, injecting K/V via Attention Hooks.

**Q: What's the difference between DKI and RAG?**  
A:

-   **RAG**: Concatenates retrieved content at token level, consumes context window
-   **DKI**: Injects K/V at attention level, doesn't consume token budget
-   They are complementary: RAG handles external knowledge, DKI handles user-level memory

**Q: What changes does the upstream app need to make?**  
A:

1. Provide adapter config file (specify database connection and field mapping)
2. Remove RAG/Prompt engineering code
3. Pass `user_id` and raw input when calling DKI API

**Q: What if the upstream app's database doesn't have vector indexes?**  
A: DKI supports dynamic vector processing, just configure `vector_search.type: dynamic`. Three strategies supported:

-   `lazy`: Compute embedding on-demand
-   `batch`: Batch pre-computation
-   `hybrid`: BM25 initial filtering + embedding reranking (recommended)

**Q: What's the difference between preference and history injection?**  
A:

-   **Preferences**: K/V injection at negative positions via Attention Hook, implicit influence, cached
-   **History**: Suffix prompt at positive positions, standard token concatenation, explicit reference, dynamic

**Q: How to integrate DKI into existing systems?**  
A:

```python
from dki.core.dki_plugin import DKIPlugin

# Create from config file (recommended)
dki = await DKIPlugin.from_config(
    model_adapter=your_model_adapter,
    adapter_config_path="config/adapter_config.yaml",
)

# Only pass user_id and raw input when calling
response = await dki.chat(
    query="User's raw input",
    user_id="user_123",
    session_id="session_456",
)
```

**Q: Does DKI need to consider distributed deployment?**  
A: No. DKI as an LLM plugin only reads user config and message data to complete injection. Distributed deployment is the responsibility of the LLM engine and upstream application. DKI itself is stateless (except for preference K/V cache) and can scale horizontally with LLM instances.

**Q: Production deployment recommendations?**  
A:

1. Enable hybrid injection
2. Set preference Î± to 0.4 (conservative)
3. Configure adapter to connect to upstream app's database
4. Monitor injection rate and latency
5. Adjust alpha and cache strategy based on metrics

### Latest Optimizations (v2.5)

#### Memory Trigger

Detects memory-related signals in user input, deciding "what to remember":

```python
# Supports 5 trigger types
- META_COGNITIVE: "what we just discussed", "you said earlier"
- STATE_CHANGE: "I changed my mind", "let me add"
- LONG_TERM_VALUE: "please remember I like...", "I'm vegetarian"
- RECALL_REQUEST: "what did we discuss recently"
- OPINION_QUERY: "do you have new thoughts"

# Rules are configurable, can be enhanced with classifier later
trigger = MemoryTrigger(language="auto")
result = trigger.detect("What did we just talk about?")
```

#### Reference Resolver

Parses referential expressions in user input, determines history recall scope:

```python
# Recall turns are externally configurable
resolver = ReferenceResolver(config=ReferenceResolverConfig(
    last_few_turns=5,    # "just now" recalls 5 turns
    recent_turns=20,     # "recently" recalls 20 turns
))

# Supports runtime dynamic updates
dki.update_reference_resolver_config(
    just_now_turns=3,
    recently_turns=15,
)
```

#### Why Rolling Summary Is Not Needed

Unlike ChatGPT/Claude/Grok, DKI **does not need** Rolling Summary:

| Approach           | Reason                                  | DKI Alternative                            |
| ------------------ | --------------------------------------- | ------------------------------------------ |
| RAG+Prompt         | Context window limit, needs compression | K/V injection doesn't consume context      |
| Rolling Summary    | Information loss from compression       | Memory Trigger for precise recall          |
| Summary Generation | Extra LLM call overhead                 | Reference Resolver for on-demand retrieval |

### Roadmap

**Completed**:

-   [x] Core DKI implementation (Attention Hook K/V injection)
-   [x] vLLM/LLaMA/DeepSeek/GLM adapters
-   [x] Hybrid injection strategy (preference K/V + history suffix)
-   [x] Config-driven adapter (SQLAlchemy dynamic table mapping)
-   [x] Dynamic vector processing (BM25 + Embedding hybrid search)
-   [x] Preference K/V cache (memory level)
-   [x] Monitoring API (stats/logs/health)
-   [x] Vue3 Example Frontend UI
-   [x] Experiment framework
-   [x] Memory Trigger
-   [x] Reference Resolver (configurable recall turns)

**In Progress**:

-   [ ] Stance State Machine
-   [ ] Classifier-enhanced Memory Trigger

**Future Work**:

-   [ ] Redis distributed cache integration
-   [ ] Attention visualization tools
-   [ ] Multi-modal extension (image/audio memory)
-   [ ] LangChain/LlamaIndex integration

---

## ğŸ”® Future Work Directions

### 1. Redis Distributed Cache Integration â­ Recommended Priority

**Current State**: Preference K/V cache is memory-only, effective for single instance.

**Optimization Goal**: Integrate Redis for cross-instance shared caching.

**Why Redis Integration Is the Most Important Optimization**:

One of DKI's core advantages is **preference K/V cache reuse**â€”after computing K/V on the first turn, subsequent requests use the cache directly, reducing latency by 43.7%. However, the current memory cache has a critical limitation: **only effective for single instance**.

In production environments, LLM services are typically multi-instance deployments (load balanced). If user requests are routed to different instances, cache misses occur, and DKI's core advantage is significantly diminished:

| Deployment Mode | Cache Hit Rate | DKI Advantage      |
| --------------- | -------------- | ------------------ |
| Single Instance | ~70%           | Full benefit       |
| 2 Instances     | ~35%           | Halved             |
| 4 Instances     | ~17.5%         | Greatly reduced    |
| N Instances     | ~70%/N         | Nearly ineffective |

**After Redis Integration**: Regardless of instance count, cache hit rate remains ~70%, DKI advantage fully preserved.

**Core Value**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Current Architecture (Single-Instance Cache)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  LLM Instance 1          LLM Instance 2          LLM Instance 3         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ DKI Plugin  â”‚         â”‚ DKI Plugin  â”‚         â”‚ DKI Plugin  â”‚        â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚ â”‚ Memory  â”‚ â”‚         â”‚ â”‚ Memory  â”‚ â”‚         â”‚ â”‚ Memory  â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ user_001â”‚ â”‚         â”‚ â”‚ user_002â”‚ â”‚         â”‚ â”‚ user_003â”‚ â”‚        â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                         â”‚
â”‚  Problems:                                                              â”‚
â”‚  - user_001 request to Instance 2 = cache miss, K/V recomputation       â”‚
â”‚  - Cache hit rate decreases with more instances                         â”‚
â”‚  - Cannot achieve true horizontal scaling                               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Optimized Architecture (Redis Distributed Cache)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  LLM Instance 1          LLM Instance 2          LLM Instance 3         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ DKI Plugin  â”‚         â”‚ DKI Plugin  â”‚         â”‚ DKI Plugin  â”‚        â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚
â”‚  â”‚ â”‚ L1 Mem  â”‚ â”‚         â”‚ â”‚ L1 Mem  â”‚ â”‚         â”‚ â”‚ L1 Mem  â”‚ â”‚        â”‚
â”‚  â”‚ â”‚ (Hot)   â”‚ â”‚         â”‚ â”‚ (Hot)   â”‚ â”‚         â”‚ â”‚ (Hot)   â”‚ â”‚        â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                       â”‚                       â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                 â–¼                                       â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚      Redis Cluster      â”‚                          â”‚
â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                          â”‚
â”‚                    â”‚  â”‚L2 Distributed Cacheâ”‚ â”‚                          â”‚
â”‚                    â”‚  â”‚user_001, user_002  â”‚ â”‚                          â”‚
â”‚                    â”‚  â”‚user_003, ...       â”‚ â”‚                          â”‚
â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                                         â”‚
â”‚  Benefits:                                                              â”‚
â”‚  - Any instance can hit cache                                           â”‚
â”‚  - Cache hit rate unaffected by instance count                          â”‚
â”‚  - True horizontal scaling support                                      â”‚
â”‚  - Cache persistence, survives restarts                                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Plan**:

```python
# Configuration example
cache:
  type: tiered  # memory | redis | tiered
  tiered:
    l1:
      type: memory
      max_size_mb: 512
      ttl: 300  # 5 minutes
    l2:
      type: redis
      host: redis-cluster.example.com
      port: 6379
      password: ${REDIS_PASSWORD}
      db: 0
      ttl: 3600  # 1 hour
      key_prefix: "dki:kv:"
      serialization: msgpack  # compressed serialization
```

**Feasibility Assessment**:

| Dimension            | Rating      | Notes                                                                      |
| -------------------- | ----------- | -------------------------------------------------------------------------- |
| Technical Complexity | â­â­ Medium | Redis client mature, main work is K/V Tensor serialization                 |
| Performance Impact   | â­â­ Medium | Network latency ~1-5ms, but avoids K/V recomputation (~50-200ms)           |
| Benefit              | â­â­â­ High | Essential for multi-instance deployment, significant cache hit improvement |
| Dependencies         | â­ Low      | Only redis-py, optional dependency                                         |

**Key Technical Points**:

1. **K/V Tensor Serialization**: Use `msgpack` + `numpy` compression to reduce network transfer
2. **Cache Invalidation Strategy**: Active invalidation on preference change, TTL as fallback
3. **Hot Data Local Cache**: L1 memory cache for high-frequency users, reduce Redis access

### 2. Attention Visualization Tools

**Goal**: Debug K/V injection effects, understand injection impact on attention distribution.

**Design Plan**:

```python
# Visualization API
from dki.visualization import AttentionVisualizer

visualizer = AttentionVisualizer(dki_plugin)

# Generate attention heatmap
heatmap = visualizer.generate_heatmap(
    query="Recommend a restaurant",
    user_id="user_001",
    layer_indices=[0, 12, 24],  # Visualize specific layers
)

# Compare before/after injection
comparison = visualizer.compare_injection(
    query="Recommend a restaurant",
    user_id="user_001",
    show_diff=True,
)

# Export as HTML report
visualizer.export_report("attention_analysis.html")
```

**Feasibility Assessment**:

| Dimension            | Rating      | Notes                                                     |
| -------------------- | ----------- | --------------------------------------------------------- |
| Technical Complexity | â­â­ Medium | Need to hook attention weights, visualization libs mature |
| Performance Impact   | â­â­â­ High | Only enabled during debugging, disabled in production     |
| Benefit              | â­â­ Medium | Valuable for debugging and paper presentation             |
| Dependencies         | â­ Low      | matplotlib, plotly as optional dependencies               |

### 3. Multi-Modal Extension (Image/Audio Memory)

**Goal**: Support images and audio as part of user memory.

**Design Plan**:

```python
# Multi-modal preferences
preferences = [
    {"type": "text", "content": "likes minimalist style"},
    {"type": "image", "content": "user_avatar.jpg", "embedding": [...]},
    {"type": "audio", "content": "voice_sample.wav", "embedding": [...]},
]

# Multi-modal K/V injection
# Images/audio first converted to embedding via encoder, then K/V computed
```

**Feasibility Assessment**:

| Dimension            | Rating      | Notes                                                         |
| -------------------- | ----------- | ------------------------------------------------------------- |
| Technical Complexity | â­â­â­ High | Needs multi-modal encoders, K/V computation method adjustment |
| Performance Impact   | â­â­â­ High | Image/audio encoding overhead is large                        |
| Benefit              | â­â­ Medium | Valuable for specific scenarios (e.g., virtual assistants)    |
| Dependencies         | â­â­â­ High | Requires CLIP, Whisper, etc.                                  |

**Recommendation**: Long-term goal, lower priority.

### 4. LangChain/LlamaIndex Integration

**Goal**: Package DKI as LangChain/LlamaIndex module to expand ecosystem.

**Design Plan**:

```python
# LangChain integration
from langchain_dki import DKIMemory

memory = DKIMemory(
    adapter_config_path="config/adapter_config.yaml",
)

chain = ConversationChain(
    llm=llm,
    memory=memory,  # DKI as Memory module
)

# LlamaIndex integration
from llama_index_dki import DKIRetriever

retriever = DKIRetriever(
    adapter_config_path="config/adapter_config.yaml",
)

query_engine = index.as_query_engine(
    retriever=retriever,  # DKI as Retriever
)
```

**Feasibility Assessment**:

| Dimension            | Rating      | Notes                                           |
| -------------------- | ----------- | ----------------------------------------------- |
| Technical Complexity | â­â­ Medium | Need to adapt LangChain/LlamaIndex interfaces   |
| Performance Impact   | â­ Low      | Wrapper layer only, no extra overhead           |
| Benefit              | â­â­â­ High | Expand user base, lower adoption barrier        |
| Dependencies         | â­â­ Medium | langchain, llama-index as optional dependencies |

**Recommendation**: Medium priority, implement after core features stabilize.

### 5. FlashAttention-3 Integration â­ Implemented

**Goal**: Integrate FlashAttention-3/2 to optimize attention computation for K/V injection.

**Current Status**: âœ… Basic framework implemented with automatic backend detection and graceful degradation.

**Core Value**:

| Scenario                 | Standard | FlashAttention-3 | Improvement |
| ------------------------ | -------- | ---------------- | ----------- |
| Preference K/V Compute   | ~50ms    | ~15ms            | **70%â†“**    |
| Inference with Injection | ~200ms   | ~80ms            | **60%â†“**    |
| GPU Memory Usage         | 24GB     | 14GB             | **42%â†“**    |

**GPU Support Matrix**:

| GPU Type  | Backend  | Support Status            |
| --------- | -------- | ------------------------- |
| H100/H200 | FA3      | âœ… Full support (optimal) |
| A100      | FA2      | âœ… Supported              |
| RTX 4090  | FA2      | âœ… Supported              |
| V100      | Standard | âš ï¸ Fallback to standard   |

**Usage**:

```python
from dki.attention import FlashAttentionConfig

# Enable FlashAttention
model_adapter.enable_flash_attention(
    config=FlashAttentionConfig(
        backend="auto",  # Auto-detect GPU
        kv_injection={"chunk_size": 1024},
    )
)

# View statistics
stats = model_adapter.get_flash_attn_stats()
```

**Configuration Example**:

```yaml
# config/config.yaml
flash_attention:
    enabled: true
    backend: "auto" # auto | fa3 | fa2 | standard
    fa3:
        use_fp8: false
        enable_async: true
    kv_injection:
        enabled: true
        strategy: "prepend"
        chunked: true
        chunk_size: 1024
```

For detailed documentation, see: [FlashAttention-3 Integration](docs/FlashAttention3_Integration.md)

### Priority Ranking

| Priority | Optimization Direction           | Reason                                       |
| -------- | -------------------------------- | -------------------------------------------- |
| P0       | FlashAttention-3 Integration     | âœ… Implemented, significant performance gain |
| P1       | Redis Distributed Cache          | âœ… Implemented, essential for multi-instance |
| P2       | Attention Visualization          | Valuable for debugging and papers            |
| P3       | LangChain/LlamaIndex Integration | Expand ecosystem, but not core               |
| P4       | Multi-Modal Extension            | High complexity, specific scenarios          |

> ğŸ“‹ For the detailed optimization roadmap, productization value analysis, and market feasibility assessment, see [DKI Optimization Roadmap & Productization Analysis](docs/DKI_Optimization_Roadmap.md).

### Additional Value of Redis Integration

Beyond solving multi-instance caching, Redis integration brings additional value:

1. **Cache Persistence**

    - Cache survives service restarts
    - Reduces K/V recomputation on cold start

2. **Cache Warming**

    - Pre-compute K/V for high-frequency users
    - Batch import historical user preferences

3. **Cache Monitoring**

    - Redis provides rich monitoring metrics
    - Analyze cache hit rate, memory usage, etc.

4. **Cache Eviction Strategies**

    - Mature LRU/LFU strategies
    - Automatic cache capacity management

5. **Cross-Service Sharing**
    - Multiple DKI instances share the same cache
    - Can even share across different LLM services (if using same model)

**Cost Analysis**:

| Resource        | Estimate         | Notes                       |
| --------------- | ---------------- | --------------------------- |
| Redis Memory    | ~100MB/10K users | Assuming ~10KB K/V per user |
| Network Latency | 1-5ms            | Within LAN                  |
| Ops Cost        | Low              | Redis ops is mature         |

**ROI Analysis**:

```
Assumptions:
- 4 instance deployment
- Current cache hit rate ~17.5% (70%/4)
- Post-Redis cache hit rate ~70%

Benefits:
- Cache hit rate improves 4x
- Subsequent turn latency reduces ~40%
- Overall throughput improves ~30%

Costs:
- Redis instance ~$50/month (cloud service)
- Development effort ~2-3 person-days
```

### Example Frontend UI

DKI provides a **Vue3** based example frontend UI to demonstrate DKI integration:

-   **Vue 3 + TypeScript**: Type-safe modern frontend development
-   **Vite**: Fast development server and build tool
-   **Pinia**: Vue3 official state management
-   **Element Plus**: Enterprise-grade UI component library

UI Features:

-   Chat interface (with DKI metadata badges)
-   User preference management panel
-   Session history browser
-   System statistics dashboard (requires password)

**Note**: Chat UI is an example application, DKI's adapter reads its database to get user preferences and history messages.

### Acknowledgments

This project is inspired by the following research:

-   RAG ([Lewis et al., 2020](https://arxiv.org/abs/2005.11401))
-   RETRO ([Borgeaud et al., 2022](https://arxiv.org/abs/2112.04426))
-   Self-RAG ([Asai et al., 2023](https://arxiv.org/abs/2310.11511))
-   FiLM ([Perez et al., 2018](https://arxiv.org/abs/1709.07871))
-   FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135))

## ğŸ“„ Related Papers

This project is based on the paper "Dynamic KV Injection: An Attention-Level User Memory System for Large Language Models".

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines first.

---

**DKI** - Rethinking Memory Augmentation at the Attention Level
