# DKI Configuration File
# Dynamic KV Injection System Configuration

# ============ System Settings ============
system:
  name: "DKI System"
  version: "1.0.0"
  debug: true
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# ============ Database Settings ============
database:
  type: "sqlite"
  path: "./data/dki.db"
  pool_size: 5
  echo: false

# ============ Model Engine Settings ============
model:
  # Default engine: vllm, llama, deepseek, glm
  default_engine: "vllm"
  
  # Model configurations by engine
  engines:
    vllm:
      enabled: true
      model_name: "Qwen/Qwen2-7B-Instruct"
      tensor_parallel_size: 1
      max_model_len: 8192
      gpu_memory_utilization: 0.9
      trust_remote_code: true
      
    llama:
      enabled: true
      model_name: "meta-llama/Llama-3.2-3B-Instruct"
      device: "cuda"
      torch_dtype: "float16"
      load_in_8bit: false
      trust_remote_code: true
      
    deepseek:
      enabled: true
      model_name: "deepseek-ai/deepseek-llm-7b-chat"
      device: "cuda"
      torch_dtype: "float16"
      trust_remote_code: true
      
    glm:
      enabled: true
      model_name: "THUDM/glm-4-9b-chat"
      device: "cuda"
      torch_dtype: "float16"
      trust_remote_code: true

# ============ Embedding Settings ============
embedding:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cuda"
  batch_size: 32
  normalize: true

# ============ RAG Settings ============
rag:
  enabled: true
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.5
  index_type: "faiss"  # faiss, annoy

# ============ DKI Settings ============
dki:
  enabled: true
  
  # Use tiered cache instead of simple cache (recommended for production)
  use_tiered_cache: true
  
  # Memory Influence Scaling (MIS)
  # Provides continuous α ∈ [0, 1] control for injection strength
  # α = 0: vanilla LLM, α = 1: full memory influence
  mis:
    alpha_min: 0.0
    alpha_max: 1.0
    alpha_default: 0.5
    
  # Query-Conditioned Projection (QCP)
  # FiLM-style modulation: Memory-centric, query only modulates
  projection:
    rank: 64
    dropout: 0.1
    
  # Dual-Factor Gating
  # Inject = f(Uncertainty, Relevance)
  # Note: Entropy is a heuristic proxy, not rigorous uncertainty estimator
  gating:
    entropy_threshold: 0.5
    relevance_threshold: 0.7
    use_margin: true
    margin_weight: 0.3
    
  # Simple Session KV Cache (fallback)
  cache:
    enabled: true
    max_size: 100
    strategy: "weighted"  # lru, lfu, weighted
    ttl_seconds: 3600
  
  # Tiered KV Cache (Paper Section 7.4)
  # Memory hierarchy: L1(GPU) → L2(CPU) → L3(SSD) → L4(Recompute)
  tiered_cache:
    enabled: true
    l1_max_size: 10       # GPU HBM: 5-10 hot memories
    l2_max_size: 100      # CPU RAM: 50-100 warm memories
    l3_path: "./data/kv_cache"  # SSD storage path
    enable_l3: true       # Enable SSD tier
    enable_l4: true       # Enable text-only recompute fallback
    ttl_seconds: 3600
    
    # Compression settings
    l2_compression: "simple"  # simple | gear (when available)
    l3_quantization: true     # INT8 quantization for L3
    
    # Selective layer caching (optional optimization)
    # Only cache middle layers where memory injection is most effective
    selective_layers: false
    cache_layers: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
    
  # Position Encoding
  position:
    strategy: "virtual_prefix"  # virtual_prefix, actual_prefix
    
# ============ Memory Store Settings ============
memory:
  max_memories_per_session: 1000
  max_memory_length: 2048
  embedding_dim: 384
  
# ============ Server Settings ============
server:
  host: "0.0.0.0"
  port: 8080
  workers: 4
  reload: false

# ============ Experiment Settings ============
experiment:
  output_dir: "./experiment_results"
  datasets:
    - name: "persona_chat"
      path: "./data/persona_chat.json"
    - name: "hotpot_qa"
      path: "./data/hotpot_qa.json"
  metrics:
    - "memory_recall"
    - "hallucination_rate"
    - "latency"
    - "bleu"
    - "rouge"
