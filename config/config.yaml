# DKI Configuration File
# Dynamic KV Injection System Configuration

# ============ System Settings ============
system:
    name: "DKI System"
    version: "1.0.0"
    debug: true
    log_level: "INFO" # DEBUG, INFO, WARNING, ERROR

# ============ Database Settings ============
database:
    type: "sqlite"
    path: "./data/dki.db"
    pool_size: 5
    echo: false

# ============ Model Engine Settings ============
model:
    # Default engine: vllm, llama, deepseek, glm
    default_engine: "vllm"

    # Model configurations by engine
    engines:
        vllm:
            enabled: true
            model_name: "/opt/ai-demo/models/deepseek-llm-7b-chat"
            tensor_parallel_size: 1
            max_model_len: 4096
            gpu_memory_utilization: 0.9
            trust_remote_code: true

# ============ Embedding Settings ============
embedding:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    device: "cuda"
    batch_size: 32
    normalize: true

# ============ RAG Settings ============
rag:
    enabled: true
    chunk_size: 512
    chunk_overlap: 50
    top_k: 5
    similarity_threshold: 0.5
    index_type: "faiss" # faiss, annoy

# ============ DKI Settings ============
# DKI is a User-Level Cross-Session Memory System
# Scope: User preferences, session history, personal context
# NOT for: External knowledge bases, public data retrieval (use RAG)
dki:
    enabled: true
    version: "2.6"

    # Use tiered cache instead of simple cache (recommended for production)
    use_tiered_cache: true

    # ============ Injection Strategy Selection ============
    # Choose between stable (production) and full_attention (research)
    # - stable: Preferences via K/V, History via suffix prompt (default)
    # - full_attention: Both Preferences and History via K/V injection (research)
    injection_strategy: "stable" # stable | full_attention

    # ============ Hybrid Injection Strategy (Paper Section 3.9) ============
    # Layered approach: Preferences via K/V, History via suffix prompt
    # Used when injection_strategy = "stable"
    hybrid_injection:
        enabled: true
        language: "cn" # en | cn (affects prompt templates)

        # Preference Injection (K/V level, negative position)
        # Short, stable, implicit influence - like "personality"
        preference:
            enabled: true
            position_strategy: "negative" # negative | actual_prefix
            alpha: 0.4 # Lower α for background influence (0.3-0.5 recommended)
            max_tokens: 100 # Preferences should be short

        # History Injection (Token level, suffix prompt)
        # Longer, dynamic, explicit reference - like "memory"
        history:
            enabled: true
            method: "suffix_prompt" # suffix_prompt | kv_injection
            max_tokens: 500 # Can be longer than preferences
            max_messages: 10 # Recent messages to include
            prompt_template: "default" # default | custom path

    # ============ Full Attention Strategy (Research - Plan C) ============
    # Both preferences and history injected via K/V at negative positions
    # Aims for 0% context usage, but may have OOD risks
    # Used when injection_strategy = "full_attention"
    full_attention:
        enabled: true

        # Position configuration
        # C1: Fixed negative position for all injected K/V
        position_mode: "fixed_negative" # fixed_negative | constant | nope

        # Preference K/V (same as stable strategy)
        preference:
            position_start: -100 # Starting position for preference K/V
            alpha: 0.4 # α for preference injection
            max_tokens: 100

        # History K/V (NEW: injected via K/V instead of suffix)
        history:
            position_start: -500 # Starting position for history K/V
            alpha: 0.3 # Lower α for history (more tokens, more risk)
            max_tokens: 400 # Max tokens for history K/V
            max_messages: 10 # Max messages to convert to K/V

        # Global indication (minimal context usage)
        # A short hint to guide model attention to negative positions
        global_indication:
            enabled: true
            text_en: "[Memory Context Available]"
            text_cn: "[记忆上下文可用]"
            # This is the ONLY context usage in full_attention mode
            # Approximately 3-5 tokens

        # Safety settings for research
        safety:
            max_total_kv_tokens: 600 # Hard limit on total K/V tokens
            fallback_to_stable: true # Fall back to stable if K/V too large
            log_attention_patterns: true # Log attention for analysis

    # ============ Memory Source Configuration ============
    # Connect to external memory database
    memory_source:
        type: "sqlite" # sqlite | postgresql | redis | api | file
        connection: "./data/dki.db"
        table: "user_memories"
        # For API source:
        # type: "api"
        # endpoint: "https://memory-service/api/v1/memories"
        # auth: "Bearer ${MEMORY_API_KEY}"

    # ============ Memory Influence Scaling (MIS) ============
    # Provides continuous α ∈ [0, 1] control for injection strength
    # α = 0: vanilla LLM, α = 1: full memory influence
    mis:
        alpha_min: 0.0
        alpha_max: 1.0
        alpha_default: 0.5

    # ============ Query-Conditioned Projection (QCP) ============
    # FiLM-style modulation: Memory-centric, query only modulates
    projection:
        rank: 64
        dropout: 0.1

    # ============ Dual-Factor Gating ============
    # Inject = f(Uncertainty, Relevance)
    # Note: Entropy is a heuristic proxy, not rigorous uncertainty estimator
    # Design: Relevance-driven decision, entropy-modulated strength
    gating:
        entropy_threshold: 0.5
        relevance_threshold: 0.7
        entropy_ceiling: 1.0 # Max α when entropy is high
        entropy_floor: 0.5 # Max α when entropy is low
        use_margin: true
        margin_weight: 0.3

    # ============ Simple Session KV Cache (fallback) ============
    cache:
        enabled: true
        max_size: 100
        strategy: "weighted" # lru, lfu, weighted
        ttl_seconds: 3600

    # ============ Tiered KV Cache (Paper Section 7.4) ============
    # Memory hierarchy: L1(GPU) → L2(CPU) → L3(SSD) → L4(Recompute)
    # Note: This is an OPTIONAL enhancement, not required for correctness
    tiered_cache:
        enabled: true
        l1_max_size: 10 # GPU HBM: 5-10 hot memories
        l2_max_size: 100 # CPU RAM: 50-100 warm memories
        l3_path: "./data/kv_cache" # SSD storage path
        enable_l3: true # Enable SSD tier
        enable_l4: true # Enable text-only recompute fallback
        ttl_seconds: 3600

        # Compression settings
        l2_compression: "simple" # simple | gear (when available)
        l3_quantization: true # INT8 quantization for L3

        # Selective layer caching (optional optimization)
        # Only cache middle layers where memory injection is most effective
        selective_layers: false
        cache_layers:
            [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]

    # ============ Position Encoding ============
    position:
        strategy: "virtual_prefix" # virtual_prefix, actual_prefix
        # Note: Negative position mapping may introduce O.O.D. behavior
        # For short preferences (< 100 tokens), risk is minimal

    # ============ Safety Settings ============
    safety:
        max_alpha: 0.8 # Hard ceiling for α
        fallback_on_error: true # Fall back to vanilla LLM on error
        audit_logging: true # Log all injection decisions
        log_path: "./logs/dki_audit.log"

    # ============ A/B Testing ============
    ab_test:
        enabled: false
        dki_percentage: 50 # Percentage of traffic to route to DKI

# ============ Memory Store Settings ============
memory:
    max_memories_per_session: 1000
    max_memory_length: 2048
    embedding_dim: 384

# ============ Server Settings ============
server:
    host: "0.0.0.0"
    port: 8080
    workers: 4
    reload: false

# ============ User Data Adapter Settings ============
# Configure external user data source
# For detailed configuration, see: config/adapter_config.example.yaml
user_adapter:
    # Adapter type: postgresql | mysql | mongodb | redis | rest_api | memory
    type: "memory"

    # Database connection (for postgresql/mysql/mongodb)
    host: "localhost"
    port: 5432
    database: "chat_db"
    username: ""
    password: ""

    # Table/Collection names
    users_table: "users"
    messages_table: "messages"
    preferences_table: "user_preferences"

    # ============ JSON Content Extraction ============
    # If your content field stores JSON strings (e.g., raw AI responses),
    # specify the JSON key to extract the actual text content.
    #
    # Example: If content = '{"text": "Hello", "model": "gpt-4"}'
    # Set content_json_key: "text" to extract "Hello"
    #
    # Supports nested keys: "data.text", "choices.0.text"
    # If JSON parsing fails or key not found, uses raw content (safe fallback)
    messages_content_json_key: null # e.g., "text", "content", "message"
    preferences_content_json_key: null

    # REST API settings (for rest_api type)
    base_url: ""
    api_key: ""
    timeout: 30

    # Connection pool
    pool_size: 5

    # Cache settings
    enable_cache: true
    cache_ttl: 300

# ============ Preference Cache Settings ============
preference_cache:
    # L1 (Memory) settings - per-instance hot cache
    l1_max_size: 1000 # Max users in memory
    l1_max_memory_mb: 5000 # Max memory in MB

    # L2 (Redis) settings - distributed warm cache
    # IMPORTANT: Enable for multi-instance deployments!
    # Without Redis: cache hit rate = 70%/N (N = instances)
    # With Redis: cache hit rate = 70% (constant)
    l2_enabled: false # Set to true for production
    l2_ttl_seconds: 86400 # 24 hours
    l2_key_prefix: "dki:pref_kv"

    # Compression (reduces Redis memory usage by ~60%)
    enable_compression: true
    compression_level: 6

# ============ Redis Settings ============
# Distributed cache for multi-instance deployments
redis:
    enabled: true # Set to true for production
    host: "localhost"
    port: 6379
    password: "hello2026" # Leave empty if no auth
    db: 0 # Redis database number

    # Connection pool settings
    max_connections: 50 # Max connections in pool
    socket_timeout: 5.0 # Socket timeout in seconds
    socket_connect_timeout: 5.0 # Connection timeout
    retry_on_timeout: true # Auto-retry on timeout

    # Key settings
    key_prefix: "dki" # Prefix for all DKI keys
    default_ttl: 86400 # Default TTL (24 hours)

    # Compression settings
    enable_compression: true # Compress large values
    compression_level: 6 # zlib compression level (1-9)
    compression_threshold: 1024 # Only compress if > 1KB

    # Health check
    health_check_interval: 30 # Health check interval (seconds)

# ============ Non-Vectorized Data Handler Settings ============
non_vectorized_handler:
    # Default strategy: lazy | batch | hybrid
    default_strategy: "hybrid"

    # Strategy thresholds
    lazy_max_messages: 100
    batch_trigger_count: 1000

    # BM25 settings
    bm25_candidates_multiplier: 4
    bm25_min_candidates: 20

    # Embedding cache
    cache_embeddings: true
    cache_max_size: 100000
    cache_ttl_hours: 168

    # Batch computation
    batch_size: 100
    max_concurrent_batches: 4

# ============ UI Settings ============
ui:
    # Frontend framework: vue3 (planned)
    framework: "vue3"
    # Development server port
    dev_port: 3000
    # API proxy target
    api_proxy: "http://localhost:8080"

# ============ Memory Trigger Settings ============
# Detects when user input indicates memory-related signals
# See: DKI/dki/core/components/memory_trigger.py
memory_trigger:
    enabled: true
    language: "auto" # "cn", "en", "auto"

    # Use classifier enhancement (future feature)
    use_classifier: false
    classifier_model: null
    classifier_threshold: 0.7

    # Custom patterns can be added here or via external config
    # Format: list of {trigger_type, pattern, language}
    # custom_patterns:
    #   - trigger_type: "meta_cognitive"
    #     pattern: "我们之前.*?讨论"
    #     language: "cn"

# ============ Reference Resolver Settings ============
# Resolves temporal and referential expressions in user queries
# See: DKI/dki/core/components/reference_resolver.py
reference_resolver:
    # Recall turns configuration (EXTERNALLY CONFIGURABLE)
    last_few_turns: 3 # "刚刚/刚才/just now" recall turns
    recent_turns: 10 # "最近/recently" recall turns
    session_max_turns: 50 # Maximum session recall turns

    # LLM fallback (for complex references)
    use_llm_fallback: false
    llm_fallback_threshold: 0.5

    # Custom reference mappings can be added here
    # reference_mappings_cn:
    #   "自定义关键词":
    #     scope: "last_1_3_turns"
    #     type: "temporal"

# ============ FlashAttention Settings ============
# FlashAttention-3/2 integration for optimized K/V injection
# Provides significant performance improvements on supported GPUs
flash_attention:
    # Enable FlashAttention optimization
    enabled: true

    # Backend selection: auto | fa3 | fa2 | standard
    # auto: Automatically detect GPU and select best backend
    # fa3: Force FlashAttention-3 (requires H100/H200)
    # fa2: Force FlashAttention-2 (requires A100/RTX 4090)
    # standard: Use PyTorch standard attention (fallback)
    backend: "auto"

    # FlashAttention-3 specific settings (H100/H200 only)
    fa3:
        # Enable FP8 precision (further reduces memory, may affect accuracy)
        use_fp8: false
        # Enable async execution (uses TMA)
        enable_async: true
        # Enable warp specialization
        enable_warp_specialization: true
        # Software pipelining stages
        num_stages: 2

    # FlashAttention-2 settings (A100/RTX 4090)
    fa2:
        # Enable causal masking
        causal: false
        # Dropout rate (use 0 for inference)
        dropout: 0.0
        # Softmax scale (null = 1/sqrt(head_dim))
        softmax_scale: null
        # Return softmax statistics (for debugging)
        return_softmax: false

    # K/V injection optimization
    kv_injection:
        # Enable optimized K/V injection
        enabled: true
        # Injection strategy: prepend | interleave
        strategy: "prepend"
        # Use chunked injection for large K/V
        chunked: true
        # Chunk size for chunked injection
        chunk_size: 1024
        # Enable alpha blending (soft injection)
        alpha_blending: true

    # Performance profiling
    profiling:
        # Enable profiling
        enabled: false
        # Log memory usage
        log_memory: true
        # Log latency
        log_latency: true
        # Log FLOPS
        log_flops: false
        # Log output path
        log_path: "./logs/flash_attn_profile.json"

# ============ Experiment Settings ============
experiment:
    output_dir: "./experiment_results"
    datasets:
        - name: "persona_chat"
          path: "./data/persona_chat.json"
        - name: "hotpot_qa"
          path: "./data/hotpot_qa.json"
    metrics:
        - "memory_recall"
        - "hallucination_rate"
        - "latency"
        - "bleu"
        - "rouge"
