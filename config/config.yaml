# DKI Configuration File
# Dynamic KV Injection System Configuration

# ============ System Settings ============
system:
  name: "DKI System"
  version: "1.0.0"
  debug: true
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

# ============ Database Settings ============
database:
  type: "sqlite"
  path: "./data/dki.db"
  pool_size: 5
  echo: false

# ============ Model Engine Settings ============
model:
  # Default engine: vllm, llama, deepseek, glm
  default_engine: "vllm"
  
  # Model configurations by engine
  engines:
    vllm:
      enabled: true
      model_name: "Qwen/Qwen2-7B-Instruct"
      tensor_parallel_size: 1
      max_model_len: 8192
      gpu_memory_utilization: 0.9
      trust_remote_code: true
      
    llama:
      enabled: true
      model_name: "meta-llama/Llama-3.2-3B-Instruct"
      device: "cuda"
      torch_dtype: "float16"
      load_in_8bit: false
      trust_remote_code: true
      
    deepseek:
      enabled: true
      model_name: "deepseek-ai/deepseek-llm-7b-chat"
      device: "cuda"
      torch_dtype: "float16"
      trust_remote_code: true
      
    glm:
      enabled: true
      model_name: "THUDM/glm-4-9b-chat"
      device: "cuda"
      torch_dtype: "float16"
      trust_remote_code: true

# ============ Embedding Settings ============
embedding:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cuda"
  batch_size: 32
  normalize: true

# ============ RAG Settings ============
rag:
  enabled: true
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5
  similarity_threshold: 0.5
  index_type: "faiss"  # faiss, annoy

# ============ DKI Settings ============
# DKI is a User-Level Cross-Session Memory System
# Scope: User preferences, session history, personal context
# NOT for: External knowledge bases, public data retrieval (use RAG)
dki:
  enabled: true
  version: "2.5"
  
  # Use tiered cache instead of simple cache (recommended for production)
  use_tiered_cache: true
  
  # ============ Hybrid Injection Strategy (Paper Section 3.9) ============
  # Layered approach: Preferences via K/V, History via suffix prompt
  hybrid_injection:
    enabled: true
    language: "en"  # en | cn (affects prompt templates)
    
    # Preference Injection (K/V level, negative position)
    # Short, stable, implicit influence - like "personality"
    preference:
      enabled: true
      position_strategy: "negative"  # negative | actual_prefix
      alpha: 0.4                     # Lower α for background influence (0.3-0.5 recommended)
      max_tokens: 100                # Preferences should be short
    
    # History Injection (Token level, suffix prompt)
    # Longer, dynamic, explicit reference - like "memory"
    history:
      enabled: true
      method: "suffix_prompt"        # suffix_prompt | kv_injection
      max_tokens: 500                # Can be longer than preferences
      max_messages: 10               # Recent messages to include
      prompt_template: "default"     # default | custom path
  
  # ============ Memory Source Configuration ============
  # Connect to external memory database
  memory_source:
    type: "sqlite"                   # sqlite | postgresql | redis | api | file
    connection: "./data/dki.db"
    table: "user_memories"
    # For API source:
    # type: "api"
    # endpoint: "https://memory-service/api/v1/memories"
    # auth: "Bearer ${MEMORY_API_KEY}"
  
  # ============ Memory Influence Scaling (MIS) ============
  # Provides continuous α ∈ [0, 1] control for injection strength
  # α = 0: vanilla LLM, α = 1: full memory influence
  mis:
    alpha_min: 0.0
    alpha_max: 1.0
    alpha_default: 0.5
    
  # ============ Query-Conditioned Projection (QCP) ============
  # FiLM-style modulation: Memory-centric, query only modulates
  projection:
    rank: 64
    dropout: 0.1
    
  # ============ Dual-Factor Gating ============
  # Inject = f(Uncertainty, Relevance)
  # Note: Entropy is a heuristic proxy, not rigorous uncertainty estimator
  # Design: Relevance-driven decision, entropy-modulated strength
  gating:
    entropy_threshold: 0.5
    relevance_threshold: 0.7
    entropy_ceiling: 1.0             # Max α when entropy is high
    entropy_floor: 0.5               # Max α when entropy is low
    use_margin: true
    margin_weight: 0.3
    
  # ============ Simple Session KV Cache (fallback) ============
  cache:
    enabled: true
    max_size: 100
    strategy: "weighted"  # lru, lfu, weighted
    ttl_seconds: 3600
  
  # ============ Tiered KV Cache (Paper Section 7.4) ============
  # Memory hierarchy: L1(GPU) → L2(CPU) → L3(SSD) → L4(Recompute)
  # Note: This is an OPTIONAL enhancement, not required for correctness
  tiered_cache:
    enabled: true
    l1_max_size: 10       # GPU HBM: 5-10 hot memories
    l2_max_size: 100      # CPU RAM: 50-100 warm memories
    l3_path: "./data/kv_cache"  # SSD storage path
    enable_l3: true       # Enable SSD tier
    enable_l4: true       # Enable text-only recompute fallback
    ttl_seconds: 3600
    
    # Compression settings
    l2_compression: "simple"  # simple | gear (when available)
    l3_quantization: true     # INT8 quantization for L3
    
    # Selective layer caching (optional optimization)
    # Only cache middle layers where memory injection is most effective
    selective_layers: false
    cache_layers: [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
    
  # ============ Position Encoding ============
  position:
    strategy: "virtual_prefix"  # virtual_prefix, actual_prefix
    # Note: Negative position mapping may introduce O.O.D. behavior
    # For short preferences (< 100 tokens), risk is minimal
  
  # ============ Safety Settings ============
  safety:
    max_alpha: 0.8                   # Hard ceiling for α
    fallback_on_error: true          # Fall back to vanilla LLM on error
    audit_logging: true              # Log all injection decisions
    log_path: "./logs/dki_audit.log"
  
  # ============ A/B Testing ============
  ab_test:
    enabled: false
    dki_percentage: 50               # Percentage of traffic to route to DKI
    
# ============ Memory Store Settings ============
memory:
  max_memories_per_session: 1000
  max_memory_length: 2048
  embedding_dim: 384
  
# ============ Server Settings ============
server:
  host: "0.0.0.0"
  port: 8080
  workers: 4
  reload: false

# ============ Experiment Settings ============
experiment:
  output_dir: "./experiment_results"
  datasets:
    - name: "persona_chat"
      path: "./data/persona_chat.json"
    - name: "hotpot_qa"
      path: "./data/hotpot_qa.json"
  metrics:
    - "memory_recall"
    - "hallucination_rate"
    - "latency"
    - "bleu"
    - "rouge"
