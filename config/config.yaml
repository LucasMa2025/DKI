# DKI Configuration File
# Dynamic KV Injection System Configuration

# ============ System Settings ============
system:
    name: "DKI System"
    version: "1.0.0"
    debug: true
    log_level: "INFO" # DEBUG, INFO, WARNING, ERROR

# ============ Database Settings ============
database:
    type: "sqlite"
    path: "./data/dki.db"
    pool_size: 5
    echo: false

# ============ Model Engine Settings ============
model:
    # Default engine: vllm, llama, deepseek, glm
    default_engine: "vllm"

    # Model configurations by engine
    engines:
        vllm:
            enabled: true
            model_name: "/opt/ai-demo/models/deepseek-llm-7b-chat"
            # ============ GPU Parallelism ============
            # tensor_parallel_size: 模型张量并行度
            #   - 1: 单 GPU (默认，适合显存充足的场景)
            #   - 2: 双 GPU (适合 2×V100 32G 等显存受限场景)
            # 
            # 2×V100 32G 环境建议:
            #   tensor_parallel_size: 2  (模型分布到两张 GPU)
            #   gpu_memory_utilization: 0.85  (留出余量给 K/V cache)
            #   max_model_len: 4096  (7B/8B 模型足够)
            #
            # 注意: 使用 tensor_parallel_size=2 时需要:
            #   1. 设置 CUDA_VISIBLE_DEVICES=0,1
            #   2. 确保两张 GPU 通过 NVLink/PCIe 互联
            #   3. vLLM 会自动处理模型分片和通信
            tensor_parallel_size: 2
            max_model_len: 4096
            gpu_memory_utilization: 0.85
            trust_remote_code: true

# ============ Embedding Settings ============
embedding:
    model_name: "sentence-transformers/all-MiniLM-L6-v2"
    device: "cuda"
    batch_size: 32
    normalize: true

# ============ RAG Settings ============
rag:
    enabled: true
    chunk_size: 512
    chunk_overlap: 50
    top_k: 5
    similarity_threshold: 0.5
    index_type: "faiss" # faiss, annoy

# ============ DKI Settings ============
# DKI is a User-Level Cross-Session Memory System
# Scope: User preferences, session history, personal context
# NOT for: External knowledge bases, public data retrieval (use RAG)
dki:
    enabled: true
    version: "2.6"

    # Use tiered cache instead of simple cache (recommended for production)
    use_tiered_cache: true

    # ============ Injection Strategy ============
    # 系统统一使用 recall_v4 策略:
    # - 偏好: K/V 注入 (负位置)
    # - 历史: 多信号召回 + 后缀组装 (recall_v4)
    # - 事实补充: Function Call 循环
    # 
    # 注: stable 和 full_attention 策略已移除 (v3.2)
    injection_strategy: "recall_v4"

    # ============ Hybrid Injection (偏好 K/V 注入) ============
    hybrid_injection:
        enabled: true
        language: "cn" # en | cn (affects prompt templates)

        # Preference Injection (K/V level, negative position)
        # Short, stable, implicit influence - like "personality"
        preference:
            enabled: true
            position_strategy: "negative" # negative | actual_prefix
            alpha: 0.4 # Lower α for background influence (0.3-0.5 recommended)
            max_tokens: 100 # Preferences should be short

        # History 由 recall_v4 处理, 不再使用 suffix_prompt 方式
        history:
            enabled: false
            method: "recall_v4" # recall_v4 (新) — 历史由多信号召回处理
            max_tokens: 500
            max_messages: 10

    # ============ 用户隔离配置 (v3.1) ============
    isolation:
        enabled: true
        # HMAC 签名密钥 (生产环境必须从环境变量 DKI_HMAC_SECRET 读取)
        cache_key_secret: ""
        # 安全级别: strict | standard | relaxed
        security_level: "strict"
        # 验证 session 归属
        verify_session_ownership: true
        # 审计日志
        audit_enabled: true
        audit_max_entries: 10000
        # 推理上下文隔离
        enable_inference_guard: true
    
    # ============ 偏好缓存配置 ============
    preference_store:
        max_users: 1000
        ttl_seconds: 3600
    
    # ============ Executor 缓存配置 ============
    executor_cache:
        max_total: 10000
        max_per_user: 100

    # ============ Memory Source Configuration ============
    # Connect to external memory database
    memory_source:
        type: "sqlite" # sqlite | postgresql | redis | api | file
        connection: "./data/dki.db"
        table: "user_memories"
        # For API source:
        # type: "api"
        # endpoint: "https://memory-service/api/v1/memories"
        # auth: "Bearer ${MEMORY_API_KEY}"

    # ============ Memory Influence Scaling (MIS) ============
    # Provides continuous α ∈ [0, 1] control for injection strength
    # α = 0: vanilla LLM, α = 1: full memory influence
    mis:
        alpha_min: 0.0
        alpha_max: 1.0
        alpha_default: 0.5

    # ============ Query-Conditioned Projection (QCP) ============
    # FiLM-style modulation: Memory-centric, query only modulates
    projection:
        rank: 64
        dropout: 0.1

    # ============ Dual-Factor Gating ============
    # Inject = f(Uncertainty, Relevance)
    # Note: Entropy is a heuristic proxy, not rigorous uncertainty estimator
    # Design: Relevance-driven decision, entropy-modulated strength
    gating:
        entropy_threshold: 0.5
        relevance_threshold: 0.7
        entropy_ceiling: 1.0 # Max α when entropy is high
        entropy_floor: 0.5 # Max α when entropy is low
        use_margin: true
        margin_weight: 0.3

    # ============ Simple Session KV Cache (fallback) ============
    cache:
        enabled: true
        max_size: 100
        strategy: "weighted" # lru, lfu, weighted
        ttl_seconds: 3600

    # ============ Tiered KV Cache (Paper Section 7.4) ============
    # Memory hierarchy: L1(GPU) → L2(CPU) → L3(SSD) → L4(Recompute)
    # Note: This is an OPTIONAL enhancement, not required for correctness
    tiered_cache:
        enabled: true
        l1_max_size: 10 # GPU HBM: 5-10 hot memories
        l2_max_size: 100 # CPU RAM: 50-100 warm memories
        l3_path: "./data/kv_cache" # SSD storage path
        enable_l3: true # Enable SSD tier
        enable_l4: true # Enable text-only recompute fallback
        ttl_seconds: 3600

        # Compression settings
        l2_compression: "simple" # simple | gear (when available)
        l3_quantization: true # INT8 quantization for L3

        # Selective layer caching (optional optimization)
        # Only cache middle layers where memory injection is most effective
        selective_layers: false
        cache_layers:
            [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]

    # ============ Position Encoding ============
    position:
        strategy: "virtual_prefix" # virtual_prefix, actual_prefix
        # Note: Negative position mapping may introduce O.O.D. behavior
        # For short preferences (< 100 tokens), risk is minimal

    # ============ Safety Settings ============
    safety:
        max_alpha: 0.8 # Hard ceiling for α
        fallback_on_error: true # Fall back to vanilla LLM on error
        audit_logging: true # Log all injection decisions
        log_path: "./logs/dki_audit.log"

    # ============ 记忆召回策略 (v4) ============
    # 多信号融合召回 + 逐消息 Summary + Function Call 事实补充
    # 替代 flat_history 平铺方案, 支持长历史场景
    recall:
        enabled: true
        strategy: "summary_with_fact_call" # summary_with_fact_call | flat_history (旧)

        # 多信号召回
        signals:
            keyword_enabled: true       # 关键词+权重检索 (jieba)
            keyword_topk: 5             # 提取关键词数量
            keyword_method: "tfidf"     # tfidf | textrank
            vector_enabled: true        # 向量相似度检索 (FAISS)
            vector_top_k: 10            # 向量检索返回数
            vector_threshold: 0.5       # 向量相似度阈值
            reference_enabled: true     # 指代解析 (已有 ReferenceResolver)

        # 召回分数融合权重 (w1 >= w2, 事实准确优先)
        score_weights:
            keyword_weight: 0.4         # w1: 关键词权重
            vector_weight: 0.35         # w2: 向量相似度权重
            recency_weight: 0.25        # w3: 时间近度权重

        # 动态 History 预算
        budget:
            generation_reserve: 512     # 生成预留 tokens
            instruction_reserve: 150    # 限定提示预留 tokens
            min_recent_turns: 2         # 至少补充的近期完整轮次
            max_recent_turns: 5         # 最多补充的近期完整轮次

        # 逐消息 Summary 阈值
        summary:
            per_message_threshold: 200  # 单条消息超过此 token 数 → summary
            max_tokens_per_summary: 150 # 每条 summary 最大 tokens
            strategy: "extractive"      # extractive (jieba TextRank) | llm

        # Function Call (事实补充)
        fact_call:
            enabled: true
            max_rounds: 3               # 最大事实补充轮次
            max_fact_tokens: 800        # 事实段落总 token 上限
            batch_size: 5               # 每次返回的消息数 (offset+limit)

        # 提示格式化器
        prompt_formatter: "auto"        # auto | generic | deepseek | glm

    # ============ A/B Testing ============
    ab_test:
        enabled: false
        dki_percentage: 50 # Percentage of traffic to route to DKI

# ============ Memory Store Settings ============
memory:
    max_memories_per_session: 1000
    max_memory_length: 2048
    embedding_dim: 384

# ============ Server Settings ============
server:
    host: "0.0.0.0"
    port: 8080
    workers: 4
    reload: false

# ============ User Data Adapter Settings ============
# Configure external user data source
# For detailed configuration, see: config/adapter_config.example.yaml
user_adapter:
    # Adapter type: postgresql | mysql | mongodb | redis | rest_api | memory
    type: "memory"

    # Database connection (for postgresql/mysql/mongodb)
    host: "localhost"
    port: 5432
    database: "chat_db"
    username: ""
    password: ""

    # Table/Collection names
    users_table: "users"
    messages_table: "messages"
    preferences_table: "user_preferences"

    # ============ JSON Content Extraction ============
    # If your content field stores JSON strings (e.g., raw AI responses),
    # specify the JSON key to extract the actual text content.
    #
    # Example: If content = '{"text": "Hello", "model": "gpt-4"}'
    # Set content_json_key: "text" to extract "Hello"
    #
    # Supports nested keys: "data.text", "choices.0.text"
    # If JSON parsing fails or key not found, uses raw content (safe fallback)
    messages_content_json_key: null # e.g., "text", "content", "message"
    preferences_content_json_key: null

    # REST API settings (for rest_api type)
    base_url: ""
    api_key: ""
    timeout: 30

    # Connection pool
    pool_size: 5

    # Cache settings
    enable_cache: true
    cache_ttl: 300

# ============ Preference Cache Settings ============
preference_cache:
    # L1 (Memory) settings - per-instance hot cache
    l1_max_size: 1000 # Max users in memory
    l1_max_memory_mb: 5000 # Max memory in MB

    # L2 (Redis) settings - distributed warm cache
    # IMPORTANT: Enable for multi-instance deployments!
    # Without Redis: cache hit rate = 70%/N (N = instances)
    # With Redis: cache hit rate = 70% (constant)
    l2_enabled: false # Set to true for production
    l2_ttl_seconds: 86400 # 24 hours
    l2_key_prefix: "dki:pref_kv"

    # Compression (reduces Redis memory usage by ~60%)
    enable_compression: true
    compression_level: 6

# ============ Redis Settings ============
# Distributed cache for multi-instance deployments
redis:
    enabled: true # Set to true for production
    host: "localhost"
    port: 6379
    password: "hello2026" # Leave empty if no auth
    db: 0 # Redis database number

    # Connection pool settings
    max_connections: 50 # Max connections in pool
    socket_timeout: 5.0 # Socket timeout in seconds
    socket_connect_timeout: 5.0 # Connection timeout
    retry_on_timeout: true # Auto-retry on timeout

    # Key settings
    key_prefix: "dki" # Prefix for all DKI keys
    default_ttl: 86400 # Default TTL (24 hours)

    # Compression settings
    enable_compression: true # Compress large values
    compression_level: 6 # zlib compression level (1-9)
    compression_threshold: 1024 # Only compress if > 1KB

    # Health check
    health_check_interval: 30 # Health check interval (seconds)

# ============ Non-Vectorized Data Handler Settings ============
non_vectorized_handler:
    # Default strategy: lazy | batch | hybrid
    default_strategy: "hybrid"

    # Strategy thresholds
    lazy_max_messages: 100
    batch_trigger_count: 1000

    # BM25 settings
    bm25_candidates_multiplier: 4
    bm25_min_candidates: 20

    # Embedding cache
    cache_embeddings: true
    cache_max_size: 100000
    cache_ttl_hours: 168

    # Batch computation
    batch_size: 100
    max_concurrent_batches: 4

# ============ UI Settings ============
ui:
    # Frontend framework: vue3 (planned)
    framework: "vue3"
    # Development server port
    dev_port: 3000
    # API proxy target
    api_proxy: "http://localhost:8080"

# ============ Memory Trigger Settings ============
# Detects when user input indicates memory-related signals
# See: DKI/dki/core/components/memory_trigger.py
memory_trigger:
    enabled: true
    language: "auto" # "cn", "en", "auto"

    # Use classifier enhancement (future feature)
    use_classifier: false
    classifier_model: null
    classifier_threshold: 0.7

    # Custom patterns can be added here or via external config
    # Format: list of {trigger_type, pattern, language}
    # custom_patterns:
    #   - trigger_type: "meta_cognitive"
    #     pattern: "我们之前.*?讨论"
    #     language: "cn"

# ============ Reference Resolver Settings ============
# Resolves temporal and referential expressions in user queries
# See: DKI/dki/core/components/reference_resolver.py
reference_resolver:
    # Recall turns configuration (EXTERNALLY CONFIGURABLE)
    last_few_turns: 3 # "刚刚/刚才/just now" recall turns
    recent_turns: 10 # "最近/recently" recall turns
    session_max_turns: 50 # Maximum session recall turns

    # LLM fallback (for complex references)
    use_llm_fallback: false
    llm_fallback_threshold: 0.5

    # Custom reference mappings can be added here
    # reference_mappings_cn:
    #   "自定义关键词":
    #     scope: "last_1_3_turns"
    #     type: "temporal"

# ============ FlashAttention Settings ============
# FlashAttention-3/2 integration for optimized K/V injection
# Provides significant performance improvements on supported GPUs
flash_attention:
    # Enable FlashAttention optimization
    enabled: true

    # Backend selection: auto | fa3 | fa2 | standard
    # auto: Automatically detect GPU and select best backend
    # fa3: Force FlashAttention-3 (requires H100/H200)
    # fa2: Force FlashAttention-2 (requires A100/RTX 4090)
    # standard: Use PyTorch standard attention (fallback)
    backend: "auto"

    # FlashAttention-3 specific settings (H100/H200 only)
    fa3:
        # Enable FP8 precision (further reduces memory, may affect accuracy)
        use_fp8: false
        # Enable async execution (uses TMA)
        enable_async: true
        # Enable warp specialization
        enable_warp_specialization: true
        # Software pipelining stages
        num_stages: 2

    # FlashAttention-2 settings (A100/RTX 4090)
    fa2:
        # Enable causal masking
        causal: false
        # Dropout rate (use 0 for inference)
        dropout: 0.0
        # Softmax scale (null = 1/sqrt(head_dim))
        softmax_scale: null
        # Return softmax statistics (for debugging)
        return_softmax: false

    # K/V injection optimization
    kv_injection:
        # Enable optimized K/V injection
        enabled: true
        # Injection strategy: prepend | interleave
        strategy: "prepend"
        # Use chunked injection for large K/V
        chunked: true
        # Chunk size for chunked injection
        chunk_size: 1024
        # Enable alpha blending (soft injection)
        alpha_blending: true

    # Performance profiling
    profiling:
        # Enable profiling
        enabled: false
        # Log memory usage
        log_memory: true
        # Log latency
        log_latency: true
        # Log FLOPS
        log_flops: false
        # Log output path
        log_path: "./logs/flash_attn_profile.json"

# ============ Experiment Settings ============
experiment:
    output_dir: "./experiment_results"
    datasets:
        - name: "persona_chat"
          path: "./data/persona_chat.json"
        - name: "hotpot_qa"
          path: "./data/hotpot_qa.json"
    metrics:
        - "memory_recall"
        - "hallucination_rate"
        - "latency"
        - "bleu"
        - "rouge"

    # ============ Experiment Users ============
    # 实验用户及其偏好配置
    # 实验开始前会自动写入数据库 (demo_users + user_preferences 表)
    # 确保 DKI 偏好注入通过数据库可靠加载
    #
    # 使用方式:
    #   runner = ExperimentRunner()
    #   runner.setup_experiment_users()  # 自动从此配置读取
    #
    # 也可以通过代码传入自定义用户:
    #   runner.setup_experiment_users(users=[...])
    users:
        - username: "exp_user_vegetarian"
          display_name: "素食实验用户"
          preferences:
              - text: "我是素食主义者，不吃任何肉类和海鲜"
                type: "general"
                priority: 10
              - text: "我对海鲜过敏，请不要推荐任何海鲜相关的食物"
                type: "general"
                priority: 10
              - text: "我住在北京海淀区"
                type: "general"
                priority: 7

        - username: "exp_user_outdoor"
          display_name: "户外运动实验用户"
          preferences:
              - text: "我喜欢户外运动，特别是徒步和骑行"
                type: "general"
                priority: 9
              - text: "我住在上海浦东"
                type: "general"
                priority: 7
              - text: "我养了一只金毛犬叫小白"
                type: "general"
                priority: 6

        - username: "exp_user_tech"
          display_name: "技术实验用户"
          preferences:
              - text: "我是一名数据科学家，擅长Python和机器学习"
                type: "technical"
                priority: 9
              - text: "我对人工智能和深度学习很感兴趣"
                type: "domain"
                priority: 8
              - text: "我喜欢阅读科幻小说"
                type: "general"
                priority: 5

        - username: "exp_user_music"
          display_name: "音乐爱好实验用户"
          preferences:
              - text: "我是古典音乐的爱好者，特别喜欢贝多芬和莫扎特"
                type: "general"
                priority: 9
              - text: "我正在学弹吉他"
                type: "general"
                priority: 7
              - text: "我对辣椒过敏，不能吃辣的食物"
                type: "general"
                priority: 10
              - text: "我在北京工作"
                type: "general"
                priority: 6
