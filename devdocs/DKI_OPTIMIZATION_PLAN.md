# DKI ç³»ç»Ÿå®Œæ•´ä¼˜åŒ–æ–¹æ¡ˆ

> **åŸºäº `kv_cacheä¼˜åŒ–å»ºè®®.md` + `ä¼˜åŒ–å»ºè®®.md` ä¸¤ä»½æ–‡æ¡£ï¼Œç»“åˆç°æœ‰ `dki/cache` + `dki/core` å®é™…ä»£ç çš„å®¢è§‚åˆ†æä¸è½åœ°æ–¹æ¡ˆ**

---

## ã€‡ã€è¯„ä¼°æ–¹æ³•è®º

æœ¬ä¼˜åŒ–æ–¹æ¡ˆéµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. **ä»£ç äº‹å®ä¼˜å…ˆ**ï¼šæ‰€æœ‰åˆ†æåŸºäºå·²å®¡æŸ¥è¿‡çš„ä»£ç å®é™…çŠ¶æ€ï¼Œè€Œéå‡è®¾
2. **ROI æ’åº**ï¼šæŒ‰ã€Œæ”¶ç›Š / æ”¹åŠ¨é£é™©ã€æ’åºï¼Œè€ŒéæŒ‰éš¾åº¦
3. **å®¢è§‚å–èˆ**ï¼šä¸¤ä»½æ–‡æ¡£ä¸­æœ‰é‡å ã€çŸ›ç›¾æˆ–å·²è¿‡æ—¶çš„å»ºè®®ï¼Œé€æ¡æ ‡æ³¨
4. **å¯è½åœ°**ï¼šæ¯æ¡å»ºè®®ç»™å‡ºå…·ä½“ä¿®æ”¹ä½ç½®å’Œé¢„æœŸæ•ˆæœ

---

## ä¸€ã€å½“å‰ç³»ç»ŸçŠ¶æ€æ€»ç»“ï¼ˆå®¡æŸ¥åï¼‰

ç»è¿‡å®Œæ•´å®¡æŸ¥ï¼Œç³»ç»Ÿå·²ä¿®å¤çš„é—®é¢˜ï¼š

| ç±»åˆ« | å·²ä¿®å¤æ•°é‡ | å…³é”®ä¿®å¤ |
|------|-----------|---------|
| GPU æ˜¾å­˜æ³„æ¼ | 12+ | `compute_kv` / `embed` / `forward` çš„ `.detach().cpu()` + `del` |
| bfloat16 åºåˆ—åŒ– | 3 | `preference_cache` / `user_isolation` / `base.py` |
| å¹¶å‘å®‰å…¨ | 2 | `UserScopedCacheStore` çš„ `asyncio.Lock` |
| æ•°å€¼æ­£ç¡®æ€§ | 2 | åŒé‡ alpha ç¼©æ”¾ä¿®å¤ã€score fusion æƒé‡å½’ä¸€åŒ– |
| æ¶æ„ç¼ºé™· | 5+ | `close()` æ¥å£ã€marker è¿‡æ»¤ã€token ä¼°ç®—ç­‰ |

**å½“å‰ç³»ç»Ÿæ˜¯"æ­£ç¡®ä½†ä¿å®ˆ"çš„çŠ¶æ€**ï¼Œä¼˜åŒ–ç©ºé—´é›†ä¸­åœ¨æ€§èƒ½å’Œå·¥ç¨‹æ•ˆç‡ã€‚

---

## äºŒã€ä¼˜åŒ–é¡¹å®Œæ•´æ¸…å•ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”´ P0 â€” å¿…åšï¼ˆä¸Šçº¿å‰ / é«˜å¹¶å‘å‰ï¼‰

---

#### P0-1ï¼š`torch.cuda.empty_cache()` ç­–ç•¥æ€§è£å‰ª

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` é—®é¢˜ 1

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰ `empty_cache()` åˆ†å¸ƒï¼ˆç» grep ç»Ÿè®¡ï¼‰ï¼š

| æ–‡ä»¶ | è°ƒç”¨æ¬¡æ•° | ä½ç½® |
|------|---------|------|
| `vllm_adapter.py` | 6 | `compute_kv` / `embed` / `forward_with_kv_injection` |
| `deepseek_adapter.py` | 3 | `compute_kv` / `embed` / `forward` |
| `glm_adapter.py` | 3 | åŒä¸Š |
| `llama_adapter.py` | 3 | åŒä¸Š |
| `preference_cache.py` | 2 | `_compute_kv` æ­£å¸¸ + å¼‚å¸¸è·¯å¾„ |
| `user_isolation.py` | 2 | åŒä¸Š |
| `hybrid_injector.py` | 2 | `_get_or_compute_preference_kv` |
| `injection_executor.py` | 1 | `_get_preference_kv` |
| `embedding_service.py` | 1 | embedding è®¡ç®—å |

**æ€»è®¡çº¦ 24 å¤„è°ƒç”¨**ï¼Œç»å¤§å¤šæ•°åœ¨æ­£å¸¸è·¯å¾„ä¸Šã€‚

**é—®é¢˜**ï¼šPyTorch caching allocator çš„è®¾è®¡æ˜¯ `del tensor` åæ˜¾å­˜å›åˆ° allocator æ± ï¼Œ`empty_cache()` å°†æ± ä¸­å—å½’è¿˜ CUDA driverã€‚é¢‘ç¹è°ƒç”¨å¯¼è‡´ï¼š
- allocator é¢‘ç¹æŠ–åŠ¨ â†’ åç»­ `to(device)` é‡æ–° malloc
- éšå¼ PCIe / driver sync â†’ æ‹‰é•¿ latency
- å¤šå¹¶å‘æ—¶ allocator é”ç«äº‰

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```
ä¿ç•™è§„åˆ™:
âœ… del tensor              â†’ å¿…é¡»ä¿ç•™ï¼ˆæ‰€æœ‰ä½ç½®ï¼‰
âœ… empty_cache()           â†’ ä»…ä¿ç•™åœ¨ä»¥ä¸‹ä½ç½®:
   1. OOM recovery çš„ except å—ä¸­
   2. Executor.execute() çš„ finally å—ï¼ˆè¯·æ±‚è¾¹ç•Œç»Ÿä¸€æ¸…ç†ï¼‰
   3. æ¨¡å‹å¸è½½ / ç³»ç»Ÿå…³é—­æ—¶

åˆ é™¤è§„åˆ™:
âŒ compute_kv æ­£å¸¸è·¯å¾„ä¸­çš„ empty_cache    â†’ åˆ é™¤
âŒ embed æ­£å¸¸è·¯å¾„ä¸­çš„ empty_cache          â†’ åˆ é™¤
âŒ forward_with_kv_injection æ­£å¸¸è·¯å¾„      â†’ åˆ é™¤
âŒ _get_preference_kv æ­£å¸¸è·¯å¾„             â†’ åˆ é™¤
âŒ _compute_kv (preference_cache) æ­£å¸¸è·¯å¾„ â†’ åˆ é™¤
```

**å…·ä½“ä¿®æ”¹**ï¼š

1. **æ‰€æœ‰ model adapter** (`vllm/deepseek/glm/llama_adapter.py`)ï¼š
   - `compute_kv`ï¼šä¿ç•™ `del outputs`ï¼Œåˆ é™¤æ­£å¸¸è·¯å¾„ `empty_cache()`ï¼Œä¿ç•™ except å—ä¸­çš„
   - `embed`ï¼šåŒä¸Š
   - `forward_with_kv_injection`ï¼šä¿ç•™ `del past_kv, outputs`ï¼Œåˆ é™¤æ­£å¸¸è·¯å¾„ `empty_cache()`

2. **`injection_executor.py`**ï¼š
   - `_get_preference_kv`ï¼šåˆ é™¤ L780 çš„ `empty_cache()`
   - **æ–°å¢**ï¼šåœ¨ `execute()` æ–¹æ³•çš„ `finally` å—ä¸­æ·»åŠ ç»Ÿä¸€æ¸…ç†ç‚¹ï¼š
   ```python
   async def execute(self, plan, ...):
       try:
           # ... ç°æœ‰é€»è¾‘ ...
       except Exception as e:
           # ... ç°æœ‰é™çº§é€»è¾‘ ...
       finally:
           # è¯·æ±‚è¾¹ç•Œç»Ÿä¸€æ¸…ç†
           if torch.cuda.is_available():
               torch.cuda.empty_cache()
   ```

3. **`preference_cache.py` / `user_isolation.py`**ï¼š
   - `_compute_kv`ï¼šåˆ é™¤æ­£å¸¸è·¯å¾„ `empty_cache()`ï¼Œä¿ç•™ except å—ä¸­çš„

**é¢„æœŸæ•ˆæœ**ï¼š
- å•è¯·æ±‚ latency é™ä½ 5-15msï¼ˆå‡å°‘ sync ç‚¹ï¼‰
- å¹¶å‘åœºæ™¯ä¸‹ allocator é”ç«äº‰æ˜¾è‘—å‡å°‘
- GPU æ˜¾å­˜åˆ©ç”¨ç‡æ›´ç¨³å®šï¼ˆallocator å¤ç”¨æ›´é«˜æ•ˆï¼‰

**é£é™©**ï¼šä½ã€‚`del tensor` å·²ç¡®ä¿å¼•ç”¨é‡Šæ”¾ï¼Œallocator è‡ªç„¶å¤ç”¨ã€‚

---

#### P0-2ï¼šExecutor å†…ç½®ç¼“å­˜åŠ  LRU / å®¹é‡ä¸Šé™

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 4

**ç°çŠ¶åˆ†æ**ï¼š

`injection_executor.py` L127ï¼š
```python
self._preference_kv_cache: Dict[str, Dict[str, Tuple[Any, str]]] = {}
```

è¿™æ˜¯ä¸€ä¸ª**æ— ç•Œå­—å…¸**ï¼Œç»“æ„ä¸º `{user_id: {content_hash: (kv_entries, content_hash)}}`ã€‚

**é—®é¢˜**ï¼š
- æ—  LRU æ·˜æ±° â†’ ç”¨æˆ·æ•°å¢é•¿å CPU å†…å­˜æ— é™å¢é•¿
- æ— å®¹é‡ä¸Šé™ â†’ æ— æ³•é¢„æµ‹å†…å­˜å ç”¨
- `clear_preference_cache` åªæä¾›å…¨é‡æ¸…é™¤ï¼Œæ— è‡ªåŠ¨æ·˜æ±°

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

ä¸º Executor çš„ `_preference_kv_cache` æ·»åŠ åŒå±‚é™åˆ¶ï¼š

```python
from collections import OrderedDict

class BoundedUserKVCache:
    """
    æœ‰ç•Œçš„ç”¨æˆ·çº§ KV ç¼“å­˜
    
    åŒå±‚é™åˆ¶:
    - max_users: æœ€å¤§ç”¨æˆ·æ•° (LRU æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„ç”¨æˆ·)
    - max_entries_per_user: æ¯ç”¨æˆ·æœ€å¤§æ¡ç›®æ•°
    """
    def __init__(self, max_users: int = 500, max_entries_per_user: int = 5):
        self._max_users = max_users
        self._max_entries_per_user = max_entries_per_user
        self._cache: OrderedDict[str, OrderedDict[str, Tuple]] = OrderedDict()
    
    def get(self, user_id: str, content_hash: str):
        if user_id in self._cache:
            self._cache.move_to_end(user_id)  # LRU touch
            return self._cache[user_id].get(content_hash)
        return None
    
    def put(self, user_id: str, content_hash: str, value):
        if user_id not in self._cache:
            if len(self._cache) >= self._max_users:
                # æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„ç”¨æˆ·
                evicted_uid, evicted_data = self._cache.popitem(last=False)
                del evicted_data  # é‡Šæ”¾ CPU tensor
            self._cache[user_id] = OrderedDict()
        
        self._cache.move_to_end(user_id)
        user_cache = self._cache[user_id]
        
        if len(user_cache) >= self._max_entries_per_user:
            user_cache.popitem(last=False)
        
        user_cache[content_hash] = value
    
    def clear(self, user_id=None):
        if user_id:
            self._cache.pop(user_id, None)
        else:
            self._cache.clear()
```

**ä¿®æ”¹ä½ç½®**ï¼š`injection_executor.py` L127

**é¢„æœŸæ•ˆæœ**ï¼š
- CPU å†…å­˜å ç”¨å¯é¢„æµ‹ï¼ˆmax_users Ã— max_entries Ã— kv_sizeï¼‰
- è‡ªåŠ¨æ·˜æ±°å†·ç”¨æˆ·çš„ KV æ•°æ®
- é˜²æ­¢é•¿æœŸè¿è¡Œåå†…å­˜æ³„æ¼

**é£é™©**ï¼šæä½ã€‚çº¯å¢é‡æ”¹åŠ¨ã€‚

---

#### P0-3ï¼šFact Call é‡æ„ä¸º Planner-only

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` æœ€ç»ˆå»ºè®® #1 + Fact Call é‡æ„è¯¦ç»†æ–¹æ¡ˆ

**ç°çŠ¶åˆ†æ**ï¼š

`injection_executor.py` L459-661 çš„ `_execute_fact_call_loop` å®ç°ï¼š
- `for round_idx in range(max_rounds)` å¾ªç¯
- æ¯è½®ï¼šdetect â†’ retrieve â†’ format â†’ re-inferï¼ˆå®Œæ•´æ¨ç†ï¼‰
- æ¯è½®æ¨ç†éƒ½éœ€è¦ `_get_preference_kv` + `forward_with_kv_injection`

**é—®é¢˜**ï¼ˆä¸‰ä¸ªå±‚é¢ï¼‰ï¼š

1. **æ¶æ„èŒè´£é”™ä½**ï¼šExecutor å˜æˆäº†æ§åˆ¶å™¨ï¼Œè¿å Planner/Executor åˆ†ç¦»åŸåˆ™
2. **GPU æˆæœ¬ä¸å¯é¢„æµ‹**ï¼šforward æ¬¡æ•° = 1~Nï¼ŒKV æ³¨å…¥æ¬¡æ•° = Nï¼Œå³°å€¼æ˜¾å­˜ä¸å¯é¢„æµ‹
3. **ç›‘æ§/å¯è§£é‡Šæ€§è¢«ç ´å**ï¼šä¸€æ¬¡ç”¨æˆ·è¯·æ±‚ â†’ N æ¬¡ `model.forward`ï¼Œlatency ä¸å¯é¢„æµ‹

**ä¼˜åŒ–æ–¹æ¡ˆï¼ˆåˆ†ä¸¤é˜¶æ®µï¼‰**ï¼š

**é˜¶æ®µ Aï¼ˆçŸ­æœŸï¼Œä½é£é™©ï¼‰ï¼šç¡¬é™åˆ¶ + ç›‘æ§**

ä¸é‡æ„æ¶æ„ï¼Œä½†åŠ å¼ºçº¦æŸï¼š

```python
# injection_executor.py - _execute_fact_call_loop ä¿®æ”¹
# 1. æ·»åŠ æ€» latency ç¡¬é™åˆ¶
MAX_FACT_CALL_LATENCY_MS = 2000  # 2 ç§’ç¡¬é™åˆ¶

for round_idx in range(max_rounds):
    elapsed_ms = (time.time() - loop_start) * 1000
    if elapsed_ms > MAX_FACT_CALL_LATENCY_MS:
        logger.warning(f"Fact call latency budget exceeded: {elapsed_ms:.0f}ms")
        break
    # ... ç°æœ‰é€»è¾‘ ...

# 2. åœ¨ ExecutionResult ä¸­æ·»åŠ ç›‘æ§å­—æ®µ
result.metadata["fact_call_latency_ms"] = elapsed_ms
result.metadata["fact_call_rounds_actual"] = round_idx + 1
```

**é˜¶æ®µ Bï¼ˆä¸­æœŸï¼Œé«˜æ”¶ç›Šï¼‰ï¼šPlanner-only é‡æ„**

å®Œæ•´å®ç° Planner-side Fact Detectionï¼š

1. **æ–°å¢æ•°æ®ç»“æ„**ï¼ˆ`injection_plan.py`ï¼‰ï¼š
```python
@dataclass
class FactRequirement:
    trace_id: str
    source: str           # "history" | "memory"
    query: str
    offset: int = 0
    limit: int = 5
    confidence: float = 1.0

# InjectionPlan æ‰©å±•
@dataclass
class InjectionPlan:
    # ... ç°æœ‰å­—æ®µ ...
    fact_blocks: List[str] = field(default_factory=list)
    fact_tokens: int = 0
    fact_strategy: str = "none"  # "planner_only" | "none"
```

2. **Planner æ–°å¢æ–¹æ³•**ï¼ˆ`injection_planner.py`ï¼‰ï¼š
```python
def _detect_fact_requirements(
    self, query: str, recall_results: List[ChatMessage]
) -> List[FactRequirement]:
    """åŸºäºè§„åˆ™ + å¬å›ä¿¡å·æ£€æµ‹äº‹å®éœ€æ±‚ï¼ˆæ›¿ä»£ LLM detectï¼‰"""
    requirements = []
    
    # è§„åˆ™ 1ï¼šæ˜ç¡®æ—¶é—´æŒ‡ä»£
    if any(kw in query for kw in ["ä¸Šæ¬¡", "ä¹‹å‰", "å‰é¢", "earlier", "last time"]):
        requirements.append(FactRequirement(
            trace_id=generate_trace_id(),
            source="history",
            query=query,
            limit=5,
        ))
    
    # è§„åˆ™ 2ï¼šrecall_v4 å¬å›ç½®ä¿¡åº¦ä¸è¶³
    if recall_results and self._low_recall_confidence(recall_results):
        requirements.append(FactRequirement(
            trace_id=generate_trace_id(),
            source="history",
            query="è¡¥å……ç›¸å…³ä¸Šä¸‹æ–‡",
            limit=3,
            confidence=0.6,
        ))
    
    return requirements
```

3. **Executor ç®€åŒ–**ï¼šåˆ é™¤æ•´ä¸ª `_execute_fact_call_loop`ï¼Œ`execute()` ä¸­ç§»é™¤ Fact Call åˆ†æ”¯

**é¢„æœŸæ•ˆæœ**ï¼š
- GPU æˆæœ¬ï¼šforward æ¬¡æ•°ä» 1~N é™ä¸º **æ’å®š 1**
- P99 latencyï¼šæ¶ˆç­ Fact Call å¯¼è‡´çš„é•¿å°¾
- æ¶æ„æ¸…æ™°ï¼šExecutor å›å½’çº¯æ‰§è¡Œå±‚

**é£é™©**ï¼šä¸­ã€‚éœ€è¦éªŒè¯ Planner-side è§„åˆ™æ£€æµ‹çš„å¬å›è´¨é‡ä¸ä½äº LLM-based æ£€æµ‹ã€‚å»ºè®® A/B æµ‹è¯•ã€‚

**å»ºè®®**ï¼šå…ˆå®æ–½é˜¶æ®µ Aï¼ˆ1 å¤©ï¼‰ï¼Œå†è§„åˆ’é˜¶æ®µ Bï¼ˆ3-5 å¤©ï¼‰ã€‚

---

#### P0-4ï¼š`history_alpha` åŠ¨æ€è¡°å‡

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` é—®é¢˜ 1

**ç°çŠ¶åˆ†æ**ï¼š

`injection_planner.py` L538ï¼š
```python
hist_alpha = 1.0  # å›ºå®šå€¼
```

`AlphaProfile` ä¸­ `history_alpha: float = 1.0` æ˜¯ç¡¬ç¼–ç ã€‚

**é—®é¢˜**ï¼š
- å†å² suffix token å¾ˆå¤šæ—¶ï¼Œæ¨¡å‹ 100% ä¿¡ä»»å†å² â†’ è¿‡åº¦"é¡ºç€æ—§è¯è¯´"
- æ–° query çš„åˆ›æ–°æ€§/ç‹¬ç«‹æ€§ä¸‹é™
- é•¿å¯¹è¯ä¸­"äººæ ¼ç²˜æ»"é£é™©

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
# injection_planner.py - _compute_alpha_profile ä¿®æ”¹
import math

def _compute_alpha_profile(self, strategy, preferences, relevant_history, force_alpha):
    # ... ç°æœ‰ force_alpha å’Œ pref_alpha é€»è¾‘ ...
    
    # åŠ¨æ€ history_alphaï¼šå†å²è¶Šé•¿ï¼Œæƒé‡è¶Šä½
    history_tokens = sum(
        self._estimate_tokens(msg.content) for msg in relevant_history
    )
    
    if history_tokens > 0:
        # å¯¹æ•°è¡°å‡ï¼š512 token ä»¥ä¸‹ alpha=1.0ï¼Œä¹‹åé€æ¸è¡°å‡åˆ° 0.3
        hist_alpha = max(
            0.3,
            min(1.0, 1.0 - math.log(max(history_tokens / 512, 1.0)) * 0.3)
        )
    else:
        hist_alpha = 1.0
    
    return AlphaProfile(
        preference_alpha=pref_alpha,
        history_alpha=hist_alpha,
    )
```

**è¡°å‡æ›²çº¿**ï¼š

| history_tokens | history_alpha |
|---------------|---------------|
| 0-512 | 1.0 |
| 1024 | ~0.79 |
| 2048 | ~0.58 |
| 4096 | ~0.37 |
| 8192+ | 0.30 (ä¸‹é™) |

**æ³¨æ„**ï¼šå½“å‰ `history_alpha` åœ¨ `recall_v4` ç­–ç•¥ä¸­è¯­ä¹‰ä¸º"åç¼€ç»„è£…æƒé‡"ï¼Œå®é™…ä¸Š suffix æ˜¯ç›´æ¥æ‹¼æ¥åˆ° prompt çš„ï¼Œalpha å¹¶ä¸ç”¨äºç¼©æ”¾ suffix æ–‡æœ¬ã€‚å› æ­¤ï¼Œæ­¤ä¼˜åŒ–çš„å®é™…æ•ˆæœå–å†³äº `history_alpha` æ˜¯å¦è¢« Executor ç”¨äºæŸç§å½¢å¼çš„åŠ æƒã€‚

**å®¢è§‚è¯„ä¼°**ï¼šå¦‚æœ `history_alpha` ä»…åœ¨ stable ç­–ç•¥çš„ K/V æ³¨å…¥ä¸­ä½¿ç”¨ï¼Œè€Œ recall_v4 çš„å†å²æ˜¯çº¯ suffix æ‹¼æ¥ï¼ˆalpha=1.0 æ„å‘³ç€ä¸åšä»»ä½•ç¼©æ”¾ï¼‰ï¼Œé‚£ä¹ˆæ­¤ä¼˜åŒ–**ä»…å¯¹ stable å›é€€è·¯å¾„æœ‰æ•ˆ**ã€‚å¯¹äº recall_v4ï¼Œæ›´æœ‰æ•ˆçš„æ–¹å¼æ˜¯åœ¨ `SuffixBuilder` ä¸­é€šè¿‡ **token budget** æ¥æ§åˆ¶å†å²é•¿åº¦ï¼ˆè§ P1-1ï¼‰ã€‚

**ä¿®æ”¹ä½ç½®**ï¼š`injection_planner.py` L515-555

**é£é™©**ï¼šä½ã€‚è¡°å‡å‡½æ•°æœ‰ä¸Šä¸‹é™ä¿æŠ¤ã€‚

---

### ğŸŸ¡ P1 â€” æ¨èï¼ˆè§„æ¨¡åŒ–å‰ï¼‰

---

#### P1-1ï¼š`recall_limit` â†’ Token Budgetï¼ˆè½¯é¢„ç®—ï¼‰

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` å»ºè®® 4

**ç°çŠ¶åˆ†æ**ï¼š

`injection_planner.py` L228ï¼š
```python
context.recall_limit = reference_result.recall_turns or 10  # ç¦»æ•£æ¡æ•°
```

`MultiSignalRecall` å’Œ `SuffixBuilder` ä½¿ç”¨æ¡æ•°é™åˆ¶ã€‚

**é—®é¢˜**ï¼š
- æ¡æ•°é™åˆ¶æ˜¯ç²—ç²’åº¦çš„ï¼š5 æ¡çŸ­æ¶ˆæ¯ vs 5 æ¡é•¿æ¶ˆæ¯ï¼Œtoken å·®å¼‚å·¨å¤§
- æ— æ³•ç²¾ç¡®æ§åˆ¶æœ€ç»ˆ prompt é•¿åº¦
- å¯èƒ½å¯¼è‡´ context window æº¢å‡ºæˆ–æµªè´¹

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
# QueryContext æ‰©å±•
@dataclass
class QueryContext:
    recall_limit: int = 10
    recall_token_budget: int = 2048  # æ–°å¢ï¼štoken è½¯é¢„ç®—
    # ... å…¶ä»–å­—æ®µ ...

# injection_planner.py - analyze_query ä¿®æ”¹
def analyze_query(self, query):
    context = QueryContext()
    # ... ç°æœ‰é€»è¾‘ ...
    
    # Token budget è®¡ç®—
    base_budget = 2048  # é»˜è®¤
    if context.reference_resolved:
        # æ˜ç¡®å¼•ç”¨æ—¶ç»™æ›´å¤šé¢„ç®—
        base_budget = int(base_budget * 1.5)
    if context.trigger_confidence > 0.8:
        # é«˜ç½®ä¿¡åº¦ trigger æ—¶ç»™æ›´å¤šé¢„ç®—
        base_budget = int(base_budget * 1.2)
    
    context.recall_token_budget = base_budget
    return context
```

ç„¶ååœ¨ `SuffixBuilder` ä¸­ä½¿ç”¨ `recall_token_budget` è€Œé `recall_limit` æ¥æ§åˆ¶ç»„è£…ã€‚

**é¢„æœŸæ•ˆæœ**ï¼š
- Prompt é•¿åº¦å¯é¢„æµ‹
- æ›´ç²¾ç¡®çš„ context window åˆ©ç”¨
- ä¸æ¨¡å‹ max_length å¯¹é½

**é£é™©**ï¼šä½-ä¸­ã€‚éœ€è¦è°ƒæ•´ `SuffixBuilder` å’Œ `MultiSignalRecall` çš„æ¥å£ã€‚

---

#### P1-2ï¼šMemoryTrigger confidence å½±å“ AlphaProfile

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` å»ºè®® 5

**ç°çŠ¶åˆ†æ**ï¼š

`injection_planner.py` L206-210ï¼š
```python
context.trigger_confidence = trigger_result.confidence
# è®°å½•äº† confidenceï¼Œä½†ä»æœªç”¨äºå½±å“ alpha
```

**é—®é¢˜**ï¼šMemoryTrigger æ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦çš„è®°å¿†è§¦å‘ï¼ˆå¦‚"è¿˜è®°å¾—æˆ‘å–œæ¬¢ä»€ä¹ˆå—"ï¼‰ï¼Œä½†åå¥½æ³¨å…¥å¼ºåº¦ä¸å˜ã€‚

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
# _compute_alpha_profile ä¸­æ·»åŠ 
if context and context.memory_triggered and context.trigger_confidence > 0.5:
    # é«˜ç½®ä¿¡åº¦ trigger æ—¶å¢å¼ºåå¥½æ³¨å…¥
    confidence_boost = min(context.trigger_confidence, 1.0)
    pref_alpha = min(
        pref_alpha * (0.7 + 0.3 * confidence_boost),  # æœ€å¤šæå‡ 30%
        0.7  # ä»å— override_cap çº¦æŸ
    )
```

**é¢„æœŸæ•ˆæœ**ï¼š
- ç”¨æˆ·ä¸»åŠ¨è¯¢é—®åå¥½æ—¶ï¼Œåå¥½æ³¨å…¥æ›´å¼º
- æ™®é€šå¯¹è¯æ—¶ä¿æŒé»˜è®¤å¼ºåº¦
- æå‡åå¥½ç›¸å…³å¯¹è¯çš„è´¨é‡

**é£é™©**ï¼šæä½ã€‚å— `override_cap` ä¿æŠ¤ã€‚

---

#### P1-3ï¼šåå¥½æ–‡æœ¬æœ¬èº«ç¼“å­˜ï¼ˆAdapter å±‚ï¼‰

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` å»ºè®® 2

**ç°çŠ¶åˆ†æ**ï¼š

`dki_plugin.py` L579ï¼š
```python
preferences = await self.data_adapter.get_user_preferences(user_id)
```

æ¯æ¬¡ `chat()` è°ƒç”¨éƒ½ä¼šæŸ¥è¯¢æ•°æ®åº“è·å–åå¥½æ–‡æœ¬ã€‚åå¥½æ˜¯**ä½é¢‘å˜æ›´æ•°æ®**ï¼Œä½†æ¯æ¬¡éƒ½èµ° DB æŸ¥è¯¢ã€‚

**é—®é¢˜**ï¼š
- åå¥½ KV æœ‰ä¸‰çº§ç¼“å­˜ï¼ˆL1/L2/L3ï¼‰ï¼Œä½†åå¥½**æ–‡æœ¬æœ¬èº«**æ²¡æœ‰ç¼“å­˜
- æ¯æ¬¡ chat éƒ½æŸ¥ DB â†’ P95 å»¶è¿Ÿæ¥æº
- åç›´è§‰ï¼šKVï¼ˆè¡ç”Ÿç‰©ï¼‰æœ‰ç¼“å­˜ï¼Œæ–‡æœ¬ï¼ˆæºæ•°æ®ï¼‰æ²¡æœ‰

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

åœ¨ `DKIPlugin` ä¸­æ·»åŠ åå¥½æ–‡æœ¬ç¼“å­˜ï¼š

```python
# dki_plugin.py
from functools import lru_cache
import time

class DKIPlugin:
    def __init__(self, ...):
        # ... ç°æœ‰åˆå§‹åŒ– ...
        self._preference_text_cache: Dict[str, Tuple[List, float]] = {}
        self._preference_cache_ttl = 300  # 5 åˆ†é’Ÿ TTL
    
    async def _get_cached_preferences(self, user_id: str):
        """å¸¦ TTL çš„åå¥½æ–‡æœ¬ç¼“å­˜"""
        now = time.time()
        if user_id in self._preference_text_cache:
            cached, cached_at = self._preference_text_cache[user_id]
            if now - cached_at < self._preference_cache_ttl:
                return cached
        
        preferences = await self.data_adapter.get_user_preferences(user_id)
        self._preference_text_cache[user_id] = (preferences, now)
        return preferences
```

**ä¿®æ”¹ä½ç½®**ï¼š`dki_plugin.py` L579ï¼Œå°† `get_user_preferences` æ›¿æ¢ä¸º `_get_cached_preferences`

**é¢„æœŸæ•ˆæœ**ï¼š
- 5 åˆ†é’Ÿå†…åŒä¸€ç”¨æˆ·çš„é‡å¤æŸ¥è¯¢ä¸èµ° DB
- P95 å»¶è¿Ÿé™ä½ 5-20msï¼ˆå–å†³äº DB å»¶è¿Ÿï¼‰
- åå¥½æ›´æ–°åæœ€å¤š 5 åˆ†é’Ÿç”Ÿæ•ˆï¼ˆå¯æ¥å—ï¼‰

**é£é™©**ï¼šæä½ã€‚TTL ä¿è¯æœ€ç»ˆä¸€è‡´æ€§ã€‚

---

#### P1-4ï¼šKV ç›‘æ§æŒ‡æ ‡å¢å¼º

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 12 + `ä¼˜åŒ–å»ºè®®.md` å»ºè®® 6

**ç°çŠ¶åˆ†æ**ï¼š

`ExecutionResult` å·²æœ‰ `inference_latency_ms`ã€`preference_cache_hit`ã€`preference_cache_tier`ã€‚

**ç¼ºå¤±æŒ‡æ ‡**ï¼š
- `kv_bytes_cpu`ï¼šCPU ä¸Šçš„ KV ç¼“å­˜å¤§å°
- `kv_bytes_gpu_peak`ï¼šæ¨ç†æœŸé—´ GPU KV å³°å€¼
- `kv_transfer_latency_ms`ï¼šCPUâ†’GPU ä¼ è¾“è€—æ—¶
- `allocator_fragmentation`ï¼šallocator ç¢ç‰‡ç‡

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

```python
# injection_plan.py - ExecutionResult æ‰©å±•
@dataclass
class ExecutionResult:
    # ... ç°æœ‰å­—æ®µ ...
    
    # KV ç›‘æ§ (æ–°å¢)
    kv_bytes_cpu: int = 0
    kv_bytes_gpu_peak: int = 0
    kv_transfer_latency_ms: float = 0.0
    kv_layers_count: int = 0

# injection_executor.py - _execute_with_kv_injection ä¸­æ·»åŠ 
if preference_kv:
    result.kv_layers_count = len(preference_kv)
    result.kv_bytes_cpu = sum(
        e.key.nelement() * e.key.element_size() +
        e.value.nelement() * e.value.element_size()
        for e in preference_kv
    )
```

**é£é™©**ï¼šæä½ã€‚çº¯è§‚æµ‹æ€§æ”¹åŠ¨ã€‚

---

### ğŸŸ¢ P2 â€” ä¸­æœŸä¼˜åŒ–ï¼ˆæ”¶ç›Šé«˜ï¼Œæ”¹åŠ¨å¤§ï¼‰

---

#### P2-1ï¼šKV æ‰“åŒ…ï¼ˆKV Packingï¼‰

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 2

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰ KV å­˜å‚¨ä¸º `List[KVCacheEntry]`ï¼Œæ¯ä¸ª entry åŒ…å«ä¸€å±‚çš„ key å’Œ valueï¼š
```python
# 32 å±‚æ¨¡å‹ â†’ 32 ä¸ª KVCacheEntry
# æ¯ä¸ª entry: key=[1, H, T, D], value=[1, H, T, D]
```

CPU â‡„ GPU ä¼ è¾“æ˜¯é€å±‚è¿›è¡Œçš„ï¼š
```python
# injection_executor.py L764-769
cpu_kv_entries = [
    KVCacheEntry(key=e.key.cpu(), value=e.value.cpu(), layer_idx=e.layer_idx)
    for e in kv_entries
]
```

**é—®é¢˜**ï¼š
- 32 å±‚ Ã— 2 tensor Ã— memcpy = 64 æ¬¡ CPUâ‡„GPU ä¼ è¾“
- æ¯æ¬¡ `.to()` æ˜¯ä¸€æ¬¡ launch + sync ç‚¹
- Redis åºåˆ—åŒ–ä¹Ÿæ˜¯é€å±‚ metadata

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

æ–°å¢ `PackedKV` æ•°æ®ç»“æ„ï¼š

```python
# models/base.py æ–°å¢
@dataclass
class PackedKV:
    """
    æ‰“åŒ…çš„ KV ç¼“å­˜ - å°†æ‰€æœ‰å±‚çš„ KV åˆå¹¶ä¸ºå•ä¸€ tensor
    
    Shape:
    - keys:   [L, H, T, D]  (L=layers, H=heads, T=tokens, D=head_dim)
    - values: [L, H, T, D]
    
    ä¼˜åŠ¿:
    - CPUâ†’GPU æ‹·è´: 64 æ¬¡ â†’ 2 æ¬¡
    - Redis åºåˆ—åŒ–: 64 æ¬¡ metadata â†’ 1 æ¬¡
    - Alpha scaling: per-layer â†’ ä¸€æ¬¡ vectorized
    """
    keys: torch.Tensor      # [L, H, T, D]
    values: torch.Tensor     # [L, H, T, D]
    num_layers: int
    dtype: torch.dtype
    
    @classmethod
    def from_entries(cls, entries: List[KVCacheEntry]) -> "PackedKV":
        """ä» List[KVCacheEntry] æ‰“åŒ…"""
        if not entries:
            raise ValueError("Empty entries")
        
        sorted_entries = sorted(entries, key=lambda e: e.layer_idx)
        keys = torch.stack([e.key.squeeze(0) for e in sorted_entries])
        values = torch.stack([e.value.squeeze(0) for e in sorted_entries])
        
        return cls(
            keys=keys,
            values=values,
            num_layers=len(sorted_entries),
            dtype=keys.dtype,
        )
    
    def to_entries(self) -> List[KVCacheEntry]:
        """è§£åŒ…ä¸º List[KVCacheEntry]"""
        return [
            KVCacheEntry(
                key=self.keys[i].unsqueeze(0),
                value=self.values[i].unsqueeze(0),
                layer_idx=i,
            )
            for i in range(self.num_layers)
        ]
    
    def to(self, device) -> "PackedKV":
        """æ•´ä½“æ¬ç§»åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆå•æ¬¡ä¼ è¾“ï¼‰"""
        return PackedKV(
            keys=self.keys.to(device),
            values=self.values.to(device),
            num_layers=self.num_layers,
            dtype=self.dtype,
        )
    
    def scale_values(self, alpha: float) -> "PackedKV":
        """Vectorized alpha scalingï¼ˆinplaceï¼‰"""
        self.values.mul_(alpha)
        return self
```

**æ”¹åŠ¨èŒƒå›´**ï¼š
1. `models/base.py`ï¼šæ–°å¢ `PackedKV`
2. `injection_executor.py`ï¼š`_get_preference_kv` è¿”å› `PackedKV`ï¼Œç¼“å­˜ `PackedKV`
3. `preference_cache.py`ï¼šåºåˆ—åŒ–/ååºåˆ—åŒ– `PackedKV`
4. Model adaptersï¼š`forward_with_kv_injection` æ¥å— `PackedKV` æˆ– `List[KVCacheEntry]`

**é¢„æœŸæ•ˆæœ**ï¼š

| é¡¹ç›® | ç°åœ¨ | æ‰“åŒ…å |
|------|------|--------|
| CPUâ†’GPU æ‹·è´ | 64 æ¬¡ | **2 æ¬¡** |
| Redis åºåˆ—åŒ– | 64 æ¬¡ metadata | **1 æ¬¡** |
| allocator ç¢ç‰‡ | é«˜ | **æ˜¾è‘—é™ä½** |
| alpha scaling | per-layer loop | **ä¸€æ¬¡ `mul_`** |

**é£é™©**ï¼šä¸­ã€‚éœ€è¦ä¿®æ”¹å¤šä¸ªæ–‡ä»¶çš„æ¥å£ï¼Œå»ºè®®æ–°æ—§æ ¼å¼å¹¶å­˜è¿‡æ¸¡ã€‚

---

#### P2-2ï¼šCPU KV Tensor æ”¹ä¸º Pinned Memory

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 6

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰ CPU tensor ä½¿ç”¨é»˜è®¤çš„ pageable memoryã€‚æ¯æ¬¡ `.to(device)` éœ€è¦ï¼š
1. CPU pageable â†’ CPU pinnedï¼ˆéšå¼æ‹·è´ï¼‰
2. CPU pinned â†’ GPU HBMï¼ˆDMA ä¼ è¾“ï¼‰

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

åœ¨ç¼“å­˜æ—¶ç›´æ¥ä½¿ç”¨ pinned memoryï¼š

```python
# injection_executor.py - _get_preference_kv ä¿®æ”¹
cpu_kv_entries = [
    KVCacheEntry(
        key=e.key.cpu().pin_memory(),
        value=e.value.cpu().pin_memory(),
        layer_idx=e.layer_idx,
    )
    for e in kv_entries
]
```

æˆ–é…åˆ PackedKVï¼š

```python
packed = PackedKV.from_entries(kv_entries)
packed.keys = packed.keys.cpu().pin_memory()
packed.values = packed.values.cpu().pin_memory()
```

**é¢„æœŸæ•ˆæœ**ï¼š
- CPUâ†’GPU ä¼ è¾“é€Ÿåº¦æå‡ ~2xï¼ˆè·³è¿‡ pageableâ†’pinned çš„éšå¼æ‹·è´ï¼‰
- ä¸ `non_blocking=True` é…åˆæ•ˆæœæ›´ä½³

**é£é™©**ï¼šä½ã€‚pinned memory å ç”¨ç‰©ç†å†…å­˜ä¸”ä¸å¯è¢« swapï¼Œéœ€ç¡®ä¿æ€»é‡å¯æ§ã€‚é…åˆ P0-2 çš„ LRU é™åˆ¶ä½¿ç”¨ã€‚

---

#### P2-3ï¼šCPUâ†’GPU æ‹·è´ä½¿ç”¨ `non_blocking=True`

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 7

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰æ‰€æœ‰ `.to(device)` è°ƒç”¨éƒ½æ˜¯åŒæ­¥çš„ï¼ˆ`non_blocking` é»˜è®¤ä¸º `False`ï¼‰ã€‚grep ç¡®è®¤ä»£ç ä¸­æ²¡æœ‰ä»»ä½• `non_blocking` ä½¿ç”¨ã€‚

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š

åœ¨ KV ä» CPU æ¬ç§»åˆ° GPU æ—¶ä½¿ç”¨éé˜»å¡ä¼ è¾“ï¼š

```python
# injection_executor.py - _get_preference_kv ä¸­
# ä»ç¼“å­˜å–å‡ºæ—¶
kv_entries = [
    KVCacheEntry(
        key=e.key.to(self.model.device, non_blocking=True),
        value=e.value.to(self.model.device, non_blocking=True),
        layer_idx=e.layer_idx,
    )
    for e in cached_entries
]
# åœ¨å®é™…ä½¿ç”¨å‰åŒæ­¥
torch.cuda.current_stream().synchronize()
```

**æ³¨æ„**ï¼š`non_blocking=True` éœ€è¦é…åˆ pinned memory æ‰æœ‰æ„ä¹‰ã€‚å¦‚æœæº tensor ä¸åœ¨ pinned memory ä¸­ï¼ŒCUDA runtime ä¼šè‡ªåŠ¨é€€åŒ–ä¸ºåŒæ­¥æ‹·è´ã€‚

**é¢„æœŸæ•ˆæœ**ï¼š
- é…åˆ pinned memoryï¼Œä¼ è¾“ä¸è®¡ç®—å¯é‡å 
- å•è¯·æ±‚ latency é™ä½ 2-5ms

**é£é™©**ï¼šä½ã€‚éœ€è¦åœ¨æ­£ç¡®ä½ç½®æ·»åŠ  synchronizeã€‚

---

#### P2-4ï¼šRedis KV åºåˆ—åŒ–æ ¼å¼ä¼˜åŒ–

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 3 + 5

**ç°çŠ¶åˆ†æ**ï¼š

`preference_cache.py` L569-606 çš„ `_serialize_kv`ï¼š
```python
# å½“å‰æµç¨‹:
# 1. entry.key.cpu() â†’ numpy
# 2. bfloat16 â†’ float32 è½¬æ¢
# 3. numpy.tobytes()
# 4. pickle.dumps(serializable_list)
# 5. zlib.compress(data)
```

**é—®é¢˜**ï¼š
- pickle æœ‰ GIL é™åˆ¶
- numpy â†’ torch â†’ reshape â†’ copy é“¾è·¯é•¿
- bfloat16 â†’ float32 â†’ å† cast å›æ¥ï¼ˆç²¾åº¦æŸå¤± + ä½“ç§¯ç¿»å€ï¼‰
- pickle + zlib åŒé‡å¼€é”€

**ä¼˜åŒ–æ–¹æ¡ˆï¼ˆä¿å®ˆç‰ˆï¼Œæ¨èï¼‰**ï¼š

ä½¿ç”¨ `torch.save` + `io.BytesIO` æ›¿ä»£ pickle + numpyï¼š

```python
import io
import zlib

def _serialize_kv_v2(self, kv_entries):
    """ä¼˜åŒ–çš„ KV åºåˆ—åŒ– - ä½¿ç”¨ torch.save ç›´æ¥åºåˆ—åŒ–"""
    buffer = io.BytesIO()
    
    # ç›´æ¥ä¿å­˜ tensorï¼Œä¿ç•™åŸå§‹ dtypeï¼ˆåŒ…æ‹¬ bfloat16ï¼‰
    save_data = {
        'version': 2,
        'entries': [
            {
                'key': entry.key.cpu(),
                'value': entry.value.cpu(),
                'layer_idx': entry.layer_idx,
            }
            for entry in kv_entries
        ]
    }
    
    torch.save(save_data, buffer)
    data = buffer.getvalue()
    
    if self.config.enable_compression:
        data = zlib.compress(data, level=self.config.compression_level)
    
    return data

def _deserialize_kv_v2(self, data):
    """ä¼˜åŒ–çš„ KV ååºåˆ—åŒ–"""
    if self.config.enable_compression:
        data = zlib.decompress(data)
    
    buffer = io.BytesIO(data)
    save_data = torch.load(buffer, weights_only=True)
    
    from dki.models.base import KVCacheEntry
    return [
        KVCacheEntry(
            key=e['key'],
            value=e['value'],
            layer_idx=e['layer_idx'],
        )
        for e in save_data['entries']
    ]
```

**ä¼˜åŠ¿**ï¼š
- ä¿ç•™ bfloat16 åŸå§‹ç²¾åº¦ï¼ˆæ—  float32 ä¸­è½¬ï¼‰
- æ—  numpy è½¬æ¢å¼€é”€
- `torch.save` å†…éƒ¨ä½¿ç”¨é«˜æ•ˆçš„ pickle protocol + tensor ç‰¹åŒ–è·¯å¾„
- å‘åå…¼å®¹ï¼šé€šè¿‡ `version` å­—æ®µåŒºåˆ†æ–°æ—§æ ¼å¼

**é¢„æœŸæ•ˆæœ**ï¼š
- åºåˆ—åŒ–é€Ÿåº¦æå‡ ~2-3x
- bfloat16 æ¨¡å‹å­˜å‚¨ä½“ç§¯å‡åŠï¼ˆæ— éœ€è½¬ float32ï¼‰
- ååºåˆ—åŒ–æ— ç²¾åº¦æŸå¤±

**é£é™©**ï¼šä½ã€‚æ–°æ—§æ ¼å¼å¯å¹¶å­˜ã€‚

---

### ğŸ”µ P3 â€” é«˜çº§ä¼˜åŒ–ï¼ˆå‰ç»æ€§ï¼‰

---

#### P3-1ï¼šKV æ³¨å…¥æ”¯æŒ Position Remap

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 9

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰è®¾è®¡éšå«å‰æï¼šinjected KV çš„ position = prefix çš„ positionã€‚

**é—®é¢˜**ï¼š
- ä¸åŒ prompt length æ—¶ï¼Œæ³¨å…¥ KV çš„ä½ç½®ç¼–ç å¯èƒ½ä¸ä¸€è‡´
- æœªæ¥å¦‚æœæ”¯æŒ negative position / virtual prefixï¼Œéœ€è¦ RoPE rebase

**è¯„ä¼°**ï¼šå½“å‰ç³»ç»Ÿä½¿ç”¨ HuggingFace `generate` çš„ `past_key_values` æ¥å£ï¼Œä½ç½®ç¼–ç ç”±æ¨¡å‹å†…éƒ¨å¤„ç†ã€‚åªè¦ `past_key_values` çš„ sequence length ä¸ `position_ids` ä¸€è‡´ï¼Œå°±ä¸ä¼šå‡ºé—®é¢˜ã€‚

**å»ºè®®**ï¼šæš‚ä¸å®æ–½ã€‚è®°å½•ä¸ºæŠ€æœ¯å€ºåŠ¡ï¼Œåœ¨ä»¥ä¸‹åœºæ™¯è§¦å‘æ—¶å®æ–½ï¼š
- æ”¯æŒ variable-length prefix
- æ”¯æŒå¤šæº KV concat
- åˆ‡æ¢åˆ°è‡ªå®šä¹‰ attention kernel

**é£é™©**ï¼šN/Aï¼ˆæš‚ä¸å®æ–½ï¼‰

---

#### P3-2ï¼šKV Segment åŒ–ï¼ˆå¤šæ³¨å…¥æºï¼‰

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 10

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰åªæœ‰ä¸€ä¸ªæ³¨å…¥æºï¼špreference KVã€‚æœªæ¥å¯èƒ½æœ‰ï¼š
- preference A / preference B
- memory KV
- tool KV

**è¯„ä¼°**ï¼šå½“å‰å•æºè®¾è®¡è¶³å¤Ÿã€‚å¤šæº concat æ¶‰åŠï¼š
- concat é¡ºåºè¯­ä¹‰
- alpha ä¸å¯äº¤æ¢æ€§
- attention é¥±å’Œé—®é¢˜

**å»ºè®®**ï¼šæš‚ä¸å®æ–½ã€‚åœ¨éœ€è¦å¤šæºæ³¨å…¥æ—¶è®¾è®¡ `KVSegment` æŠ½è±¡ã€‚

---

#### P3-3ï¼š`compute_kv` å’Œ `forward_with_kv` Pipeline åŒ–

**æ¥æº**ï¼š`kv_cacheä¼˜åŒ–å»ºè®®.md` å»ºè®® 8

**è¯„ä¼°**ï¼šåœ¨å½“å‰æ¶æ„ä¸­ï¼Œ`compute_kv` çš„ç»“æœè¢«ç¼“å­˜ï¼Œé€šå¸¸åªåœ¨é¦–æ¬¡è¯·æ±‚æ—¶è®¡ç®—ã€‚åç»­è¯·æ±‚ç›´æ¥ä»ç¼“å­˜è¯»å– KVã€‚å› æ­¤ pipeline åŒ–çš„æ”¶ç›Šä»…åœ¨**é¦–æ¬¡è¯·æ±‚**ï¼ˆcold startï¼‰æ—¶æœ‰æ•ˆã€‚

**å»ºè®®**ï¼šä¼˜å…ˆçº§ä½ã€‚å¦‚æœ cold start latency æˆä¸ºç“¶é¢ˆï¼Œå¯ä»¥è€ƒè™‘åœ¨ `DKIPlugin.chat()` ä¸­å¹¶è¡Œå‘èµ· `compute_kv` å’Œæ•°æ®åŠ è½½ã€‚

---

#### P3-4ï¼šç”¨æˆ·åå¥½ Embedding â†’ Selective KV

**æ¥æº**ï¼š`ä¼˜åŒ–å»ºè®®.md` å»ºè®® 7

**è¯„ä¼°**ï¼šè¿™æ˜¯ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œè€Œéå·¥ç¨‹ä¼˜åŒ–ã€‚éœ€è¦ï¼š
- å¯¹åå¥½æ–‡æœ¬åš embedding
- åŸºäº query-preference ç›¸ä¼¼åº¦é€‰æ‹©æ€§æ³¨å…¥éƒ¨åˆ† KV
- éœ€è¦å®éªŒéªŒè¯æ•ˆæœ

**å»ºè®®**ï¼šä½œä¸ºå®éªŒé¡¹ç›®ï¼Œä¸çº³å…¥å·¥ç¨‹ä¼˜åŒ–è®¡åˆ’ã€‚

---

## ä¸‰ã€ä¸¤ä»½æ–‡æ¡£å»ºè®®çš„äº¤å‰åˆ†æ

| å»ºè®® | kv_cacheæ–‡æ¡£ | ä¼˜åŒ–æ–‡æ¡£ | æœ¬æ–¹æ¡ˆè¯„ä¼° | çº³å…¥ä¼˜å…ˆçº§ |
|------|-------------|---------|-----------|-----------|
| ç§»é™¤ empty_cache | âœ… å¼ºçƒˆå»ºè®® | â€” | **é‡‡çº³** | P0-1 |
| KV Packing | âœ… å»ºè®® | â€” | **é‡‡çº³** | P2-1 |
| Executor ç¼“å­˜ LRU | âœ… å»ºè®® | â€” | **é‡‡çº³** | P0-2 |
| Redis åºåˆ—åŒ–ä¼˜åŒ– | âœ… å»ºè®® | â€” | **é‡‡çº³ï¼ˆä¿å®ˆç‰ˆï¼‰** | P2-4 |
| Pinned Memory | âœ… å»ºè®® | â€” | **é‡‡çº³** | P2-2 |
| non_blocking | âœ… å»ºè®® | â€” | **é‡‡çº³ï¼ˆé…åˆ pinnedï¼‰** | P2-3 |
| Position Remap | âš ï¸ å‰ç» | â€” | **æš‚ä¸å®æ–½** | P3-1 |
| KV Segment | âš ï¸ å‰ç» | â€” | **æš‚ä¸å®æ–½** | P3-2 |
| Pipeline compute+forward | âœ… å»ºè®® | â€” | **ä¼˜å…ˆçº§ä½** | P3-3 |
| Fact Call é‡æ„ | â€” | ğŸ”¥ å¼ºçƒˆå»ºè®® | **é‡‡çº³ï¼ˆåˆ†é˜¶æ®µï¼‰** | P0-3 |
| history_alpha åŠ¨æ€è¡°å‡ | â€” | ğŸ”¥ å¿…åš | **é‡‡çº³ï¼ˆéœ€éªŒè¯è¯­ä¹‰ï¼‰** | P0-4 |
| åå¥½æ–‡æœ¬ç¼“å­˜ | â€” | ğŸ”¥ å¿…åš | **é‡‡çº³** | P1-3 |
| recall_limit â†’ token budget | â€” | æ¨è | **é‡‡çº³** | P1-1 |
| MemoryTrigger â†’ Alpha | â€” | æ¨è | **é‡‡çº³** | P1-2 |
| KV ç›‘æ§æŒ‡æ ‡ | âœ… å»ºè®® | âœ… å»ºè®® | **é‡‡çº³** | P1-4 |
| Selective KV | â€” | å¯é€‰ | **æš‚ä¸å®æ–½** | P3-4 |

---

## å››ã€å®æ–½è·¯çº¿å›¾

### Phase 1ï¼šå®‰å…¨åŠ å›ºï¼ˆ1-2 å¤©ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡è€—æ—¶ | å½±å“èŒƒå›´ |
|------|--------|---------|---------|
| P0-1ï¼šè£å‰ª empty_cache | P0 | 2h | 8 æ–‡ä»¶ |
| P0-2ï¼šExecutor LRU ç¼“å­˜ | P0 | 3h | 1 æ–‡ä»¶ |
| P0-3Aï¼šFact Call ç¡¬é™åˆ¶ | P0 | 2h | 1 æ–‡ä»¶ |
| P1-4ï¼šKV ç›‘æ§æŒ‡æ ‡ | P1 | 2h | 2 æ–‡ä»¶ |

### Phase 2ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ3-5 å¤©ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡è€—æ—¶ | å½±å“èŒƒå›´ |
|------|--------|---------|---------|
| P0-4ï¼šhistory_alpha è¡°å‡ | P0 | 2h | 1 æ–‡ä»¶ |
| P1-1ï¼šToken Budget | P1 | 4h | 3 æ–‡ä»¶ |
| P1-2ï¼šMemoryTrigger â†’ Alpha | P1 | 1h | 1 æ–‡ä»¶ |
| P1-3ï¼šåå¥½æ–‡æœ¬ç¼“å­˜ | P1 | 2h | 1 æ–‡ä»¶ |

### Phase 3ï¼šæ¶æ„å‡çº§ï¼ˆ5-10 å¤©ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | é¢„è®¡è€—æ—¶ | å½±å“èŒƒå›´ |
|------|--------|---------|---------|
| P0-3Bï¼šFact Call Planner-only | P0 | 5d | 4 æ–‡ä»¶ |
| P2-1ï¼šKV Packing | P2 | 3d | 6 æ–‡ä»¶ |
| P2-2+P2-3ï¼šPinned + non_blocking | P2 | 1d | 2 æ–‡ä»¶ |
| P2-4ï¼šRedis åºåˆ—åŒ–ä¼˜åŒ– | P2 | 2d | 2 æ–‡ä»¶ |

---

## äº”ã€æœªçº³å…¥æ–¹æ¡ˆçš„å»ºè®®åŠåŸå› 

| å»ºè®® | æ¥æº | ä¸çº³å…¥åŸå›  |
|------|------|-----------|
| KV cache allocator awareness (paged/block) | kv_cacheæ–‡æ¡£ | éœ€è¦è‡ªå®šä¹‰ CUDA allocatorï¼ŒæŠ•å…¥äº§å‡ºæ¯”ä½ |
| ç”¨æˆ·åå¥½ embedding â†’ selective KV | ä¼˜åŒ–æ–‡æ¡£ | ç ”ç©¶æ–¹å‘ï¼Œéœ€å®éªŒéªŒè¯ï¼Œéå·¥ç¨‹ä¼˜åŒ– |
| å†å²æ£€ç´¢å¼‚æ­¥åŒ– | ä¼˜åŒ–æ–‡æ¡£ | å½“å‰å·²æ˜¯ async æ¥å£ï¼Œå®é™…ç“¶é¢ˆä¸åœ¨æ­¤ |
| KV äºŒè¿›åˆ¶æ ¼å¼ï¼ˆè‡ªå®šä¹‰ headerï¼‰ | kv_cacheæ–‡æ¡£ | `torch.save` æ–¹æ¡ˆï¼ˆP2-4ï¼‰å·²è¶³å¤Ÿï¼Œè‡ªå®šä¹‰æ ¼å¼ç»´æŠ¤æˆæœ¬é«˜ |

---

## å…­ã€æ€»ç»“

**ä¸€å¥è¯**ï¼šè¿™ä¸ªç³»ç»Ÿçš„æ­£ç¡®æ€§å·²ç»è¿‡å……åˆ†éªŒè¯ï¼Œç°åœ¨éœ€è¦çš„æ˜¯**ä»"å­¦æœ¯æ­£ç¡®"èµ°å‘"ç³»ç»Ÿæœ€ä¼˜"**ã€‚

**æ ¸å¿ƒä¸‰ä»¶äº‹**ï¼š
1. **åœæ­¢æ»¥ç”¨ `empty_cache()`** â€” è®© PyTorch allocator åšå®ƒè¯¥åšçš„äº‹
2. **ç»™ Executor åŠ ä¸Šè¾¹ç•Œ** â€” LRU ç¼“å­˜ + Fact Call ç¡¬é™åˆ¶
3. **è®© KV æ•°æ®æµæ›´ç²—ç²’åº¦** â€” Packing + Pinned + non_blocking

è¿™ä¸‰ä»¶äº‹åšå®Œï¼Œç³»ç»Ÿå°±ä»"èƒ½è·‘"å˜æˆ"èƒ½æ‰›"ã€‚

---

*æ–‡æ¡£ç”Ÿæˆæ—¶é—´ï¼š2026-02-18*
*åŸºäºä»£ç å®¡æŸ¥ç‰ˆæœ¬ï¼šv3.2.0ï¼ˆå«æ‰€æœ‰å·²ä¿®å¤çš„ bugï¼‰*
